{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12073704,"sourceType":"datasetVersion","datasetId":7502148}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"cca0cbbf-a3b7-40f7-bd9a-2d60ef75627f","_cell_guid":"65a40813-acf8-462a-9e3a-881d2d48c098","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-12T06:30:05.243153Z","iopub.execute_input":"2025-06-12T06:30:05.243936Z","iopub.status.idle":"2025-06-12T06:30:05.855963Z","shell.execute_reply.started":"2025-06-12T06:30:05.243901Z","shell.execute_reply":"2025-06-12T06:30:05.854258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom datetime import datetime, timedelta\nimport math\nfrom operator import attrgetter # Added for cohort analysis lambda function\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:05.857541Z","iopub.execute_input":"2025-06-12T06:30:05.858864Z","iopub.status.idle":"2025-06-12T06:30:05.868804Z","shell.execute_reply.started":"2025-06-12T06:30:05.858800Z","shell.execute_reply":"2025-06-12T06:30:05.867725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the CSV files (update paths as needed)\nfile1 = \"/kaggle/input/crm-data-analysis/sales_order_01.04.23-01.04.24.78558153.csv\"\nfile2 = \"/kaggle/input/crm-data-analysis/sales_order_01.04.24-01.04.25.81099654.csv\"\n\n# Load the data into DataFrames\ndf1 = pd.read_csv(file1, encoding='utf-8', low_memory=False)\ndf2 = pd.read_csv(file2, encoding='utf-8', low_memory=False)\n\nprint(f\"‚úÖ Dataset 1 (2023-2024): {df1.shape}\")\nprint(f\"‚úÖ Dataset 2 (2024-2025): {df2.shape}\")\n\n# Merge the DataFrames\nmerged_df = pd.concat([df1, df2], ignore_index=True, sort=False)\nprint(f\"‚úÖ Merged dataset shape: {merged_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:05.870791Z","iopub.execute_input":"2025-06-12T06:30:05.871151Z","iopub.status.idle":"2025-06-12T06:30:25.243895Z","shell.execute_reply.started":"2025-06-12T06:30:05.871126Z","shell.execute_reply":"2025-06-12T06:30:25.242726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:25.244773Z","iopub.execute_input":"2025-06-12T06:30:25.244999Z","iopub.status.idle":"2025-06-12T06:30:25.728273Z","shell.execute_reply.started":"2025-06-12T06:30:25.244979Z","shell.execute_reply":"2025-06-12T06:30:25.727514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:25.729006Z","iopub.execute_input":"2025-06-12T06:30:25.729364Z","iopub.status.idle":"2025-06-12T06:30:26.311256Z","shell.execute_reply.started":"2025-06-12T06:30:25.729328Z","shell.execute_reply":"2025-06-12T06:30:26.310448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the DataFrames\nmerged_df = pd.concat([df1, df2], ignore_index=True, sort=False)\nprint(f\"‚úÖ Merged dataset shape: {merged_df.shape}\")\n\nmerged_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:26.312118Z","iopub.execute_input":"2025-06-12T06:30:26.312347Z","iopub.status.idle":"2025-06-12T06:30:26.903012Z","shell.execute_reply.started":"2025-06-12T06:30:26.312324Z","shell.execute_reply":"2025-06-12T06:30:26.901931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:26.904166Z","iopub.execute_input":"2025-06-12T06:30:26.904464Z","iopub.status.idle":"2025-06-12T06:30:27.933212Z","shell.execute_reply.started":"2025-06-12T06:30:26.904437Z","shell.execute_reply":"2025-06-12T06:30:27.932270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df2 = merged_df\nmerged_df2.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:27.934684Z","iopub.execute_input":"2025-06-12T06:30:27.935026Z","iopub.status.idle":"2025-06-12T06:30:27.962612Z","shell.execute_reply.started":"2025-06-12T06:30:27.934991Z","shell.execute_reply":"2025-06-12T06:30:27.961472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Keep only necessary columns\nmerged_df2 = merged_df[['Customer Email', 'Purchase Date']]\n\n# Step 2: Convert 'Purchase Date' to datetime\nmerged_df2['Purchase Date'] = pd.to_datetime(merged_df2['Purchase Date'], errors='coerce')\n\n# Step 3: Create 'Year' and 'Month-Year' columns\nmerged_df2['Year'] = merged_df2['Purchase Date'].dt.year\nmerged_df2['Month-Year'] = merged_df2['Purchase Date'].dt.to_period('M').astype(str)\n\n# Step 4: Remove null emails or dates\nmerged_df2 = merged_df2.dropna(subset=['Customer Email', 'Purchase Date'])\n\n# Step 5: Calculate Retention Rate YoY\nretention = {}\nyears = sorted(merged_df2['Year'].dropna().unique())\n\nfor i in range(len(years) - 1):\n    year_curr = years[i]\n    year_next = years[i + 1]\n\n    users_curr = set(merged_df2[merged_df2['Year'] == year_curr]['Customer Email'].unique())\n    users_next = set(merged_df2[merged_df2['Year'] == year_next]['Customer Email'].unique())\n\n    retained_users = users_curr & users_next\n    retention_rate = len(retained_users) / len(users_curr) if users_curr else 0\n\n    retention[f\"{year_curr} ‚Üí {year_next}\"] = round(retention_rate * 100, 2)\n\n# Step 6: Calculate Repeat Purchase Rate (1st to 2nd)\nrepeat_purchase = {}\nfor year in years:\n    year_data = merged_df2[merged_df2['Year'] == year]\n    purchase_counts = year_data.groupby('Customer Email').size()\n    \n    one_time = purchase_counts[purchase_counts == 1].count()\n    repeaters = purchase_counts[purchase_counts >= 2].count()\n\n    total_customers = one_time + repeaters\n    repeat_rate = repeaters / total_customers if total_customers else 0\n\n    repeat_purchase[str(year)] = round(repeat_rate * 100, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:27.966889Z","iopub.execute_input":"2025-06-12T06:30:27.967188Z","iopub.status.idle":"2025-06-12T06:30:31.970568Z","shell.execute_reply.started":"2025-06-12T06:30:27.967167Z","shell.execute_reply":"2025-06-12T06:30:31.969556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output results\nprint(\"üìä Year-over-Year Retention Rate:\")\nfor k, v in retention.items():\n    print(f\"{k}: {v}%\")\n\nprint(\"\\nüîÅ Repeat Purchase Rate:\")\nfor k, v in repeat_purchase.items():\n    print(f\"{k}: {v}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:31.971569Z","iopub.execute_input":"2025-06-12T06:30:31.972204Z","iopub.status.idle":"2025-06-12T06:30:31.980435Z","shell.execute_reply.started":"2025-06-12T06:30:31.972155Z","shell.execute_reply":"2025-06-12T06:30:31.979011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load retention data\nretention_df = pd.read_csv('/kaggle/input/crm-data-analysis/Customer Retention Rate Mehedi.csv')\nprint(f\"‚úÖ Retention data loaded: {retention_df.shape}\")\n\nretention_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:31.981467Z","iopub.execute_input":"2025-06-12T06:30:31.981825Z","iopub.status.idle":"2025-06-12T06:30:35.864175Z","shell.execute_reply.started":"2025-06-12T06:30:31.981799Z","shell.execute_reply":"2025-06-12T06:30:35.862896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retention_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:35.865287Z","iopub.execute_input":"2025-06-12T06:30:35.865676Z","iopub.status.idle":"2025-06-12T06:30:35.931040Z","shell.execute_reply.started":"2025-06-12T06:30:35.865642Z","shell.execute_reply":"2025-06-12T06:30:35.929799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monthly & Yearly Retention Repurchase Churn (Customer Retention)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load data\nretention_df = pd.read_csv('/kaggle/input/crm-data-analysis/Customer Retention Rate Mehedi.csv')\nprint(f\"‚úÖ Retention data loaded: {retention_df.shape}\")\n\n# Keep only necessary columns\nretention_df = retention_df[['EMAIL', 'FIRST_ORDER_DATE', 'LAST_ORDER_DATE']].dropna()\n\n# Convert date columns to datetime\nretention_df['FIRST_ORDER_DATE'] = pd.to_datetime(retention_df['FIRST_ORDER_DATE'], errors='coerce')\nretention_df['LAST_ORDER_DATE'] = pd.to_datetime(retention_df['LAST_ORDER_DATE'], errors='coerce')\n\n# Drop rows with invalid dates\nretention_df = retention_df.dropna(subset=['FIRST_ORDER_DATE', 'LAST_ORDER_DATE'])\n\n# Create Year and Month-Year columns\nretention_df['FIRST_YEAR'] = retention_df['FIRST_ORDER_DATE'].dt.year\nretention_df['LAST_YEAR'] = retention_df['LAST_ORDER_DATE'].dt.year\n\nretention_df['FIRST_MONTH_YEAR'] = retention_df['FIRST_ORDER_DATE'].dt.to_period('M').astype(str)\nretention_df['LAST_MONTH_YEAR'] = retention_df['LAST_ORDER_DATE'].dt.to_period('M').astype(str)\n\n# Unique customers\ncustomers_df = retention_df.drop_duplicates(subset='EMAIL')\n\n# Overall Repeat Purchase Rate (for reference)\nrepeat_df = customers_df[customers_df['FIRST_ORDER_DATE'] != customers_df['LAST_ORDER_DATE']]\noverall_repeat_purchase_rate = len(repeat_df) / len(customers_df)\nprint(f\"üîÅ Overall Repeat Purchase Rate: {overall_repeat_purchase_rate:.2%}\")\n\n# ======================= #\n# üìÖ Year-over-Year (YoY) #\n# ======================= #\nyoy_metrics = []\n\nyears = sorted(customers_df['FIRST_YEAR'].dropna().unique())\nfor i in range(1, len(years)):\n    prev_year = years[i - 1]\n    current_year = years[i]\n\n    prev_customers = set(customers_df[customers_df['FIRST_YEAR'] <= prev_year]['EMAIL'])\n    current_customers = set(customers_df[customers_df['LAST_YEAR'] == current_year]['EMAIL'])\n\n    retained = current_customers.intersection(prev_customers)\n    retention_rate = len(retained) / len(prev_customers) if prev_customers else 0\n    churn_rate = 1 - retention_rate\n\n    # Calculate dynamic Repeat Purchase Rate for YoY period\n    repeat_customers = customers_df[\n        (customers_df['FIRST_YEAR'] <= prev_year) &\n        (customers_df['LAST_YEAR'] == current_year) &\n        (customers_df['FIRST_ORDER_DATE'] != customers_df['LAST_ORDER_DATE'])\n    ]\n    year_repeat_purchase_rate = len(repeat_customers) / len(prev_customers) if prev_customers else 0\n\n    yoy_metrics.append({\n        'Year': current_year,\n        'Retention Rate': round(retention_rate, 6),\n        'Churn Rate': round(churn_rate, 6),\n        'Repeat Purchase Rate': round(year_repeat_purchase_rate, 6)\n    })\n\n# ============================ #\n# üìÜ Month-over-Month (MoM)    #\n# ============================ #\nmom_metrics = []\n\nmonths = sorted(customers_df['FIRST_MONTH_YEAR'].dropna().unique())\nfor i in range(1, len(months)):\n    prev_month = months[i - 1]\n    current_month = months[i]\n\n    prev_customers = set(customers_df[customers_df['FIRST_MONTH_YEAR'] <= prev_month]['EMAIL'])\n    current_customers = set(customers_df[customers_df['LAST_MONTH_YEAR'] == current_month]['EMAIL'])\n\n    retained = current_customers.intersection(prev_customers)\n    retention_rate = len(retained) / len(prev_customers) if prev_customers else 0\n    churn_rate = 1 - retention_rate\n\n    # Calculate dynamic Repeat Purchase Rate for MoM period\n    repeat_customers = customers_df[\n        (customers_df['FIRST_MONTH_YEAR'] <= prev_month) &\n        (customers_df['LAST_MONTH_YEAR'] == current_month) &\n        (customers_df['FIRST_ORDER_DATE'] != customers_df['LAST_ORDER_DATE'])\n    ]\n    month_repeat_purchase_rate = len(repeat_customers) / len(prev_customers) if prev_customers else 0\n\n    mom_metrics.append({\n        'Month': current_month,\n        'Retention Rate': round(retention_rate, 6),\n        'Churn Rate': round(churn_rate, 6),\n        'Repeat Purchase Rate': round(month_repeat_purchase_rate, 6)\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:35.933659Z","iopub.execute_input":"2025-06-12T06:30:35.934135Z","iopub.status.idle":"2025-06-12T06:30:44.869141Z","shell.execute_reply.started":"2025-06-12T06:30:35.934098Z","shell.execute_reply":"2025-06-12T06:30:44.862556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You can now print or save yoy_metrics and mom_metrics as needed\nprint(\"‚úÖ YoY metrics sample:\")\nyoy_metrics[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.869878Z","iopub.status.idle":"2025-06-12T06:30:44.870200Z","shell.execute_reply.started":"2025-06-12T06:30:44.870042Z","shell.execute_reply":"2025-06-12T06:30:44.870056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"‚úÖ MoM metrics sample:\")\nmom_metrics[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.871393Z","iopub.status.idle":"2025-06-12T06:30:44.871970Z","shell.execute_reply.started":"2025-06-12T06:30:44.871804Z","shell.execute_reply":"2025-06-12T06:30:44.871821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save YoY\nyoy_df = pd.DataFrame(yoy_metrics)\nyoy_df.to_csv('yearly_retention_repurchase_churn_customer_retention.csv', index=False)\nprint(\"üìÅ YoY metrics saved to 'yearly_retention_repurchase_churn_customer_retention.csv'\")\n\n# Save MoM\nmom_df = pd.DataFrame(mom_metrics)\nmom_df.to_csv('monthly_retention_repurchase_churn_customer_retention.csv', index=False)\nprint(\"üìÅ MoM metrics saved to 'monthly_retention_repurchase_churn_customer_retention.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.873994Z","iopub.status.idle":"2025-06-12T06:30:44.874759Z","shell.execute_reply.started":"2025-06-12T06:30:44.874463Z","shell.execute_reply":"2025-06-12T06:30:44.874512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. DATA CLEANING AND PREPROCESSING\nprint(\"\\nüßπ Step 3: Data cleaning and preprocessing...\")\n\n# Clean merged sales data\ndef clean_sales_data(df):\n    \"\"\"Clean and preprocess sales data\"\"\"\n    df_clean = df.copy()\n    \n    # Convert Purchase Date to datetime\n    df_clean['Purchase Date'] = pd.to_datetime(df_clean['Purchase Date'], errors='coerce')\n    \n    # Extract date components\n    df_clean['Year'] = df_clean['Purchase Date'].dt.year\n    df_clean['Month'] = df_clean['Purchase Date'].dt.month\n    df_clean['Quarter'] = df_clean['Purchase Date'].dt.quarter\n    df_clean['Day_of_Week'] = df_clean['Purchase Date'].dt.day_name()\n    df_clean['Month_Year'] = df_clean['Purchase Date'].dt.to_period('M').astype(str)\n    \n    # Clean email addresses\n    df_clean['Customer Email'] = df_clean['Customer Email'].str.lower().str.strip()\n    \n    # Handle missing values in critical columns\n    df_clean['Customer Group'] = df_clean['Customer Group'].fillna('Unknown')\n    df_clean['Status'] = df_clean['Status'].fillna('Unknown')\n    \n    # Create customer segments based on order value\n    df_clean['Order_Value_Segment'] = pd.cut(\n        df_clean['Grand Total (Base)'], \n        bins=[-np.inf, 50, 100, 200, np.inf], \n        labels=['Low (0-50)', 'Medium (50-100)', 'High (100-200)', 'Premium (200+)']\n    )\n    \n    # Remove rows with invalid purchase dates or zero/negative amounts\n    df_clean = df_clean.dropna(subset=['Purchase Date'])\n    df_clean = df_clean[df_clean['Grand Total (Base)'] > 0]\n    \n    return df_clean\n\n# Clean retention data\ndef clean_retention_data(df):\n    \"\"\"Clean and preprocess retention data\"\"\"\n    df_clean = df.copy()\n    \n    # Clean email addresses\n    df_clean['EMAIL'] = df_clean['EMAIL'].str.lower().str.strip()\n    \n    # Convert date columns\n    date_columns = ['FIRST_ORDER_DATE', 'LAST_ORDER_DATE', 'ACCOUNT_CREATION_DATE']\n    for col in date_columns:\n        if col in df_clean.columns:\n            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n    \n    # Remove rows with missing email or critical data\n    df_clean = df_clean.dropna(subset=['EMAIL'])\n    df_clean = df_clean[df_clean['EMAIL'].str.contains('@', na=False)]\n    \n    # Fill missing values\n    df_clean['GENDER'] = df_clean['GENDER'].fillna('Unknown')\n    df_clean['REGION'] = df_clean['REGION'].fillna('Unknown')\n    df_clean['LOYALTY_TIER'] = df_clean['LOYALTY_TIER'].fillna('None')\n    df_clean['TOTAL_ORDER_AMOUNT'] = df_clean['TOTAL_ORDER_AMOUNT'].fillna(0)\n    \n    return df_clean\n\n# Apply cleaning\nsales_clean = clean_sales_data(merged_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.876878Z","iopub.status.idle":"2025-06-12T06:30:44.877303Z","shell.execute_reply.started":"2025-06-12T06:30:44.877110Z","shell.execute_reply":"2025-06-12T06:30:44.877128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retention_clean = clean_retention_data(retention_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.880398Z","iopub.status.idle":"2025-06-12T06:30:44.880907Z","shell.execute_reply.started":"2025-06-12T06:30:44.880690Z","shell.execute_reply":"2025-06-12T06:30:44.880712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"‚úÖ Sales data cleaned: {sales_clean.shape}\")\nprint(f\"‚úÖ Retention data cleaned: {retention_clean.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.882581Z","iopub.status.idle":"2025-06-12T06:30:44.883847Z","shell.execute_reply.started":"2025-06-12T06:30:44.883613Z","shell.execute_reply":"2025-06-12T06:30:44.883664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"‚úÖ Sales data cleaned: {sales_clean.shape}\")\nprint(f\"‚úÖ Retention data cleaned: {retention_clean.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.884915Z","iopub.status.idle":"2025-06-12T06:30:44.886095Z","shell.execute_reply.started":"2025-06-12T06:30:44.885808Z","shell.execute_reply":"2025-06-12T06:30:44.885838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sales_clean.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.887270Z","iopub.status.idle":"2025-06-12T06:30:44.887771Z","shell.execute_reply.started":"2025-06-12T06:30:44.887509Z","shell.execute_reply":"2025-06-12T06:30:44.887530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sales_clean.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.889515Z","iopub.status.idle":"2025-06-12T06:30:44.889876Z","shell.execute_reply.started":"2025-06-12T06:30:44.889720Z","shell.execute_reply":"2025-06-12T06:30:44.889737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monthly & Yearly Retention Repurchase Churn (Sales Order)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Ensure necessary columns are present and valid\nsales_df = sales_clean[['Customer Email', 'Purchase Date', 'Year']].copy()\n\n# Step 2: Drop rows with null values in essential columns\nsales_df.dropna(subset=['Customer Email', 'Purchase Date'], inplace=True)\n\n# Step 3: Calculate Year-over-Year Retention Rate\nretention = {}\nyears = sorted(sales_df['Year'].dropna().unique())\n\nfor i in range(len(years) - 1):\n    year_curr = years[i]\n    year_next = years[i + 1]\n\n    users_curr = set(sales_df[sales_df['Year'] == year_curr]['Customer Email'].unique())\n    users_next = set(sales_df[sales_df['Year'] == year_next]['Customer Email'].unique())\n\n    retained_users = users_curr & users_next\n    retention_rate = len(retained_users) / len(users_curr) if users_curr else 0\n\n    retention[f\"{year_curr} ‚Üí {year_next}\"] = round(retention_rate * 100, 2)\n\n# Step 4: Calculate Repeat Purchase Rate (1st to 2nd)\nrepeat_purchase = {}\nfor year in years:\n    year_data = sales_df[sales_df['Year'] == year]\n    purchase_counts = year_data.groupby('Customer Email').size()\n    \n    one_time = purchase_counts[purchase_counts == 1].count()\n    repeaters = purchase_counts[purchase_counts >= 2].count()\n\n    total_customers = one_time + repeaters\n    repeat_rate = repeaters / total_customers if total_customers else 0\n\n    repeat_purchase[str(year)] = round(repeat_rate * 100, 2)\n\n# Step 5: Print Results\nprint(\"üìä Year-over-Year Retention Rate:\")\nfor k, v in retention.items():\n    print(f\"{k}: {v}%\")\n\nprint(\"\\nüîÅ Repeat Purchase Rate:\")\nfor k, v in repeat_purchase.items():\n    print(f\"{k}: {v}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.891976Z","iopub.status.idle":"2025-06-12T06:30:44.892312Z","shell.execute_reply.started":"2025-06-12T06:30:44.892166Z","shell.execute_reply":"2025-06-12T06:30:44.892179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sales_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.893945Z","iopub.status.idle":"2025-06-12T06:30:44.894678Z","shell.execute_reply.started":"2025-06-12T06:30:44.894400Z","shell.execute_reply":"2025-06-12T06:30:44.894442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'sales_clean' is already loaded as a DataFrame\n\n# Step 1: Keep only necessary columns\ncols_to_keep = ['Customer Email', 'Purchase Date']\nsales = sales_clean[cols_to_keep].copy()\n\n# Step 2: Create Month_Year and Year columns if not already\nsales['Year'] = sales['Purchase Date'].dt.year\nsales['Month_Year'] = sales['Purchase Date'].dt.to_period('M').astype(str)\n\n# Step 3: Sort by Purchase Date\nsales = sales.sort_values(by=['Customer Email', 'Purchase Date'])\n\n# Step 4: Calculate total purchases per customer\npurchase_counts = sales.groupby('Customer Email')['Purchase Date'].count().reset_index()\npurchase_counts.columns = ['Customer Email', 'Purchase_Count']\n\n# Merge purchase count back to sales\nsales = sales.merge(purchase_counts, on='Customer Email')\n\n# Step 5: First Purchase Date\nfirst_purchase = sales.groupby('Customer Email')['Purchase Date'].min().reset_index()\nfirst_purchase.columns = ['Customer Email', 'First_Purchase_Date']\n\n# Merge to get first purchase month/year\nsales = sales.merge(first_purchase, on='Customer Email')\nsales['First_Purchase_Month'] = sales['First_Purchase_Date'].dt.to_period('M').astype(str)\nsales['First_Purchase_Year'] = sales['First_Purchase_Date'].dt.year\n\n# Step 6: Retention Rate (MoM and YoY)\nmonthly_customers = sales.groupby('Month_Year')['Customer Email'].nunique().reset_index()\nmonthly_customers.columns = ['Month_Year', 'Active_Customers']\n\nmonthly_customers['Prev_Month_Customers'] = monthly_customers['Active_Customers'].shift(1)\nmonthly_customers['Retention_Rate'] = (monthly_customers['Active_Customers'] / monthly_customers['Prev_Month_Customers']) * 100\n\nyearly_customers = sales.groupby('Year')['Customer Email'].nunique().reset_index()\nyearly_customers.columns = ['Year', 'Active_Customers']\n\nyearly_customers['Prev_Year_Customers'] = yearly_customers['Active_Customers'].shift(1)\nyearly_customers['Retention_Rate'] = (yearly_customers['Active_Customers'] / yearly_customers['Prev_Year_Customers']) * 100\n\n# Step 7: Repeat Purchase Rate\n# Monthly\nmonthly_repeat = sales[sales['Purchase_Count'] > 1].groupby('Month_Year')['Customer Email'].nunique().reset_index()\nmonthly_repeat.columns = ['Month_Year', 'Repeat_Customers']\nmonthly_repeat = monthly_repeat.merge(monthly_customers[['Month_Year', 'Active_Customers']], on='Month_Year', how='left')\nmonthly_repeat['Repeat_Purchase_Rate'] = (monthly_repeat['Repeat_Customers'] / monthly_repeat['Active_Customers']) * 100\n\n# Yearly\nyearly_repeat = sales[sales['Purchase_Count'] > 1].groupby('Year')['Customer Email'].nunique().reset_index()\nyearly_repeat.columns = ['Year', 'Repeat_Customers']\nyearly_repeat = yearly_repeat.merge(yearly_customers[['Year', 'Active_Customers']], on='Year', how='left')\nyearly_repeat['Repeat_Purchase_Rate'] = (yearly_repeat['Repeat_Customers'] / yearly_repeat['Active_Customers']) * 100\n\n# Step 8: Churn Rate (100 - Retention)\nmonthly_customers['Churn_Rate'] = 100 - monthly_customers['Retention_Rate']\nyearly_customers['Churn_Rate'] = 100 - yearly_customers['Retention_Rate']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.897156Z","iopub.status.idle":"2025-06-12T06:30:44.897736Z","shell.execute_reply.started":"2025-06-12T06:30:44.897467Z","shell.execute_reply":"2025-06-12T06:30:44.897488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Save as CSVs\nmonthly_output = monthly_customers.merge(monthly_repeat[['Month_Year', 'Repeat_Purchase_Rate']], on='Month_Year', how='left')\nyearly_output = yearly_customers.merge(yearly_repeat[['Year', 'Repeat_Purchase_Rate']], on='Year', how='left')\n\nmonthly_output.to_csv('monthly_retention_repurchase_churn.csv', index=False)\nyearly_output.to_csv('yearly_retention_repurchase_churn.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.899674Z","iopub.status.idle":"2025-06-12T06:30:44.900091Z","shell.execute_reply.started":"2025-06-12T06:30:44.899894Z","shell.execute_reply":"2025-06-12T06:30:44.899912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monthly_output.head(3) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.901960Z","iopub.status.idle":"2025-06-12T06:30:44.902356Z","shell.execute_reply.started":"2025-06-12T06:30:44.902151Z","shell.execute_reply":"2025-06-12T06:30:44.902172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yearly_output.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.903503Z","iopub.status.idle":"2025-06-12T06:30:44.903917Z","shell.execute_reply.started":"2025-06-12T06:30:44.903698Z","shell.execute_reply":"2025-06-12T06:30:44.903718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GAP between purchases","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Keep necessary columns\nsales_df = sales_clean[['Customer Email', 'Purchase Date']].copy()\n\n# Step 2: Convert 'Purchase Date' to datetime and drop nulls\nsales_df['Purchase Date'] = pd.to_datetime(sales_df['Purchase Date'], errors='coerce')\nsales_df.dropna(subset=['Customer Email', 'Purchase Date'], inplace=True)\n\n# Step 3: Sort data by customer and purchase date\nsales_df.sort_values(by=['Customer Email', 'Purchase Date'], inplace=True)\n\n# Step 4: For each customer, calculate the difference between 2nd and 1st purchase\ndef get_1st_2nd_gap(df):\n    if len(df) < 2:\n        return None  # Not enough purchases\n    return (df.iloc[1] - df.iloc[0]).days\n\ngap_df = sales_df.groupby('Customer Email')['Purchase Date'].apply(get_1st_2nd_gap).dropna().reset_index()\ngap_df.columns = ['Customer Email', 'Days Between 1st and 2nd Purchase']\n\n# Step 5: Display results\nprint(gap_df.head())\n\n# Optional: Save to CSV\ngap_df.to_csv('1st_to_2nd_purchase_gap.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.905485Z","iopub.status.idle":"2025-06-12T06:30:44.905812Z","shell.execute_reply.started":"2025-06-12T06:30:44.905681Z","shell.execute_reply":"2025-06-12T06:30:44.905693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"average_gap = gap_df['Days Between 1st and 2nd Purchase'].mean()\nprint(f\"\\nüìÖ Average Days Between 1st and 2nd Purchase: {round(average_gap, 2)} days\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.908178Z","iopub.status.idle":"2025-06-12T06:30:44.909073Z","shell.execute_reply.started":"2025-06-12T06:30:44.908875Z","shell.execute_reply":"2025-06-12T06:30:44.908901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. GENERATE ANALYTICS DATASETS\nprint(\"\\nüìä Step 4: Generating analytics datasets...\")\n\n# 4.1 Customer Analytics Dataset\ndef create_customer_analytics(sales_df, retention_df):\n    \"\"\"Create comprehensive customer analytics dataset\"\"\"\n    \n    # Customer aggregations from sales data\n    customer_sales = sales_df.groupby('Customer Email').agg({\n        'ID': 'count',  # Total orders\n        'Grand Total (Base)': ['sum', 'mean', 'std'],\n        'Purchase Date': ['min', 'max'],\n        'Discount Amount': 'sum',\n        'Status': lambda x: (x == 'complete').sum(),  # Completed orders\n        'Customer Group': 'first'\n    }).round(2)\n    \n    # Flatten column names\n    customer_sales.columns = [\n        'Total_Orders', 'Total_Revenue', 'Average_Order_Value', 'Order_Value_StdDev',\n        'First_Purchase_Date', 'Last_Purchase_Date', 'Total_Discount_Used',\n        'Completed_Orders', 'Customer_Group'\n    ]\n    \n    # Calculate additional metrics\n    customer_sales['Days_Between_First_Last'] = (\n        customer_sales['Last_Purchase_Date'] - customer_sales['First_Purchase_Date']\n    ).dt.days\n    \n    customer_sales['Purchase_Frequency_Days'] = (\n        customer_sales['Days_Between_First_Last'] / customer_sales['Total_Orders'].clip(lower=1)\n    ).round(2)\n    \n    customer_sales['Repeat_Customer'] = customer_sales['Total_Orders'] > 1\n    customer_sales['Customer_Lifetime_Value'] = customer_sales['Total_Revenue']\n    \n    # Customer segments\n    customer_sales['CLV_Segment'] = pd.cut(\n        customer_sales['Customer_Lifetime_Value'],\n        bins=[-np.inf, 100, 300, 500, np.inf],\n        labels=['Low', 'Medium', 'High', 'VIP']\n    )\n    \n    # Reset index to make email a column\n    customer_sales = customer_sales.reset_index()\n    customer_sales.rename(columns={'Customer Email': 'EMAIL'}, inplace=True)\n    \n    # Merge with retention data\n    customer_analytics = customer_sales.merge(\n        retention_df[['EMAIL', 'GENDER', 'REGION', 'LOYALTY_TIER', 'NEWSLETTER_FREQUENCY']],\n        on='EMAIL', how='left'\n    )\n    \n    return customer_analytics\n\n# 4.2 Cohort Analysis Dataset\ndef create_cohort_analysis(sales_df):\n    \"\"\"Create cohort analysis dataset\"\"\"\n    \n    # Define cohorts based on first purchase month\n    customer_cohorts = sales_df.groupby('Customer Email')['Purchase Date'].min().reset_index()\n    customer_cohorts.columns = ['Customer Email', 'Cohort_Month']\n    customer_cohorts['Cohort_Month'] = customer_cohorts['Cohort_Month'].dt.to_period('M')\n    \n    # Merge back with sales data\n    sales_with_cohorts = sales_df.merge(customer_cohorts, on='Customer Email')\n    sales_with_cohorts['Period_Number'] = (\n        sales_with_cohorts['Purchase Date'].dt.to_period('M') - \n        sales_with_cohorts['Cohort_Month']\n    ).apply(attrgetter('n'))\n    \n    # Create cohort table\n    cohort_data = sales_with_cohorts.groupby(['Cohort_Month', 'Period_Number'])['Customer Email'].nunique().reset_index()\n    cohort_data.columns = ['Cohort_Month', 'Period_Number', 'Customers']\n    \n    # Calculate cohort sizes\n    cohort_sizes = customer_cohorts.groupby('Cohort_Month')['Customer Email'].nunique()\n    cohort_data = cohort_data.merge(cohort_sizes.reset_index().rename(columns={'Customer Email': 'Cohort_Size'}), on='Cohort_Month')\n    \n    # Calculate retention rates\n    cohort_data['Retention_Rate'] = (cohort_data['Customers'] / cohort_data['Cohort_Size'] * 100).round(2)\n    \n    return cohort_data\n\n# Fix import for cohort analysis\nfrom operator import attrgetter\n\n# 4.3 Monthly Performance Dataset\ndef create_monthly_performance(sales_df):\n    \"\"\"Create monthly performance metrics\"\"\"\n    \n    monthly_metrics = sales_df.groupby('Month_Year').agg({\n        'ID': 'count',\n        'Customer Email': 'nunique',\n        'Grand Total (Base)': ['sum', 'mean'],\n        'Discount Amount': 'sum',\n        'Status': lambda x: (x == 'complete').sum()\n    }).round(2)\n    \n    monthly_metrics.columns = [\n        'Total_Orders', 'Unique_Customers', 'Total_Revenue', \n        'Average_Order_Value', 'Total_Discounts', 'Completed_Orders'\n    ]\n    \n    # Calculate additional metrics\n    monthly_metrics['Completion_Rate'] = (\n        monthly_metrics['Completed_Orders'] / monthly_metrics['Total_Orders'] * 100\n    ).round(2)\n    \n    monthly_metrics['Revenue_Per_Customer'] = (\n        monthly_metrics['Total_Revenue'] / monthly_metrics['Unique_Customers']\n    ).round(2)\n    \n    return monthly_metrics.reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.909823Z","iopub.status.idle":"2025-06-12T06:30:44.910108Z","shell.execute_reply.started":"2025-06-12T06:30:44.909983Z","shell.execute_reply":"2025-06-12T06:30:44.909994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.4 Product Performance Dataset (if product data available)\ndef create_product_performance(sales_df):\n    \"\"\"Create product performance metrics\"\"\"\n    \n    # Group by customer group as proxy for product categories\n    product_metrics = sales_df.groupby(['Customer Group', 'Month_Year']).agg({\n        'ID': 'count',\n        'Grand Total (Base)': ['sum', 'mean'],\n        'Customer Email': 'nunique'\n    }).round(2)\n    \n    product_metrics.columns = ['Orders', 'Revenue', 'AOV', 'Customers']\n    \n    return product_metrics.reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.911793Z","iopub.status.idle":"2025-06-12T06:30:44.912403Z","shell.execute_reply.started":"2025-06-12T06:30:44.912049Z","shell.execute_reply":"2025-06-12T06:30:44.912062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate all datasets\nprint(\"üîÑ Generating customer analytics...\")\ncustomer_analytics = create_customer_analytics(sales_clean, retention_clean)\n\ncustomer_analytics.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.914643Z","iopub.status.idle":"2025-06-12T06:30:44.915095Z","shell.execute_reply.started":"2025-06-12T06:30:44.914906Z","shell.execute_reply":"2025-06-12T06:30:44.914927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {customer_analytics.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.918013Z","iopub.status.idle":"2025-06-12T06:30:44.918405Z","shell.execute_reply.started":"2025-06-12T06:30:44.918195Z","shell.execute_reply":"2025-06-12T06:30:44.918208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Generating cohort analysis","metadata":{}},{"cell_type":"code","source":"print(\"üîÑ Generating cohort analysis...\")\ncohort_analysis = create_cohort_analysis(sales_clean)\nprint(f\"Shape: {cohort_analysis.shape}\")\ncohort_analysis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.920404Z","iopub.status.idle":"2025-06-12T06:30:44.920825Z","shell.execute_reply.started":"2025-06-12T06:30:44.920681Z","shell.execute_reply":"2025-06-12T06:30:44.920698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Generating monthly performance...\")\nmonthly_performance = create_monthly_performance(sales_clean)\n\nprint(f\"Shape: {monthly_performance.shape}\")\nmonthly_performance.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.922197Z","iopub.status.idle":"2025-06-12T06:30:44.922523Z","shell.execute_reply.started":"2025-06-12T06:30:44.922391Z","shell.execute_reply":"2025-06-12T06:30:44.922403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Generating product performance...\")\nproduct_performance = create_product_performance(sales_clean)\n\nprint(f\"Shape: {product_performance.shape}\")\nproduct_performance.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.924470Z","iopub.status.idle":"2025-06-12T06:30:44.924911Z","shell.execute_reply.started":"2025-06-12T06:30:44.924717Z","shell.execute_reply":"2025-06-12T06:30:44.924736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. GENERATE KPI SUMMARY DATASET\nprint(\"\\nüìà Step 5: Generating KPI summary...\")\n\ndef create_kpi_summary(sales_df, customer_analytics):\n    \"\"\"Create KPI summary for dashboard\"\"\"\n    \n    total_customers = customer_analytics['EMAIL'].nunique()\n    repeat_customers = customer_analytics['Repeat_Customer'].sum()\n    total_revenue = sales_df['Grand Total (Base)'].sum()\n    total_orders = len(sales_df)\n    \n    # Calculate retention rate\n    retention_rate = (repeat_customers / total_customers * 100) if total_customers > 0 else 0\n    \n    # Calculate average time between purchases for repeat customers\n    repeat_customer_data = customer_analytics[customer_analytics['Repeat_Customer']]\n    avg_time_between_purchases = repeat_customer_data['Purchase_Frequency_Days'].mean()\n    \n    # Churn rate (customers who haven't purchased in last 90 days)\n    cutoff_date = sales_df['Purchase Date'].max() - timedelta(days=90)\n    active_customers = sales_df[sales_df['Purchase Date'] >= cutoff_date]['Customer Email'].nunique()\n    churn_rate = ((total_customers - active_customers) / total_customers * 100) if total_customers > 0 else 0\n    \n    kpi_data = {\n        'Metric': [\n            'Total Customers', 'Repeat Customers', 'Customer Retention Rate (%)',\n            'Customer Churn Rate (%)', 'Total Revenue', 'Total Orders', \n            'Average Order Value', 'Avg Time Between Purchases (Days)',\n            'Repeat Purchase Rate (%)'\n        ],\n        'Value': [\n            total_customers, repeat_customers, round(retention_rate, 2),\n            round(churn_rate, 2), round(total_revenue, 2), total_orders,\n            round(total_revenue / total_orders, 2), round(avg_time_between_purchases, 2),\n            round((repeat_customers / total_customers * 100), 2)\n        ]\n    }\n    \n    return pd.DataFrame(kpi_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.926593Z","iopub.status.idle":"2025-06-12T06:30:44.926877Z","shell.execute_reply.started":"2025-06-12T06:30:44.926761Z","shell.execute_reply":"2025-06-12T06:30:44.926773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kpi_summary = create_kpi_summary(sales_clean, customer_analytics)\nprint(f\"Shape: {kpi_summary.shape}\")\nkpi_summary.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.927746Z","iopub.status.idle":"2025-06-12T06:30:44.927986Z","shell.execute_reply.started":"2025-06-12T06:30:44.927876Z","shell.execute_reply":"2025-06-12T06:30:44.927887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. SAVE ALL DATASETS\nprint(\"\\nüíæ Step 6: Saving processed datasets...\")\n\ndatasets = {\n    'merged_sales_clean.csv': sales_clean,\n    'customer_analytics.csv': customer_analytics,\n    'cohort_analysis.csv': cohort_analysis,\n    'monthly_performance.csv': monthly_performance,\n    'product_performance.csv': product_performance,\n    'kpi_summary.csv': kpi_summary,\n    'retention_data_clean.csv': retention_clean\n}\n\nfor filename, dataset in datasets.items():\n    dataset.to_csv(filename, index=False)\n    print(f\"‚úÖ Saved: {filename} ({dataset.shape})\")\n\n# 7. GENERATE DATA DICTIONARY\nprint(\"\\nüìã Step 7: Generating data dictionary...\")\n\ndata_dictionary = {\n    'File': [],\n    'Column': [],\n    'Description': [],\n    'Data_Type': []\n}\n\n# Add descriptions for key datasets\nfile_descriptions = {\n    'customer_analytics.csv': {\n        'EMAIL': 'Customer email address (primary key)',\n        'Total_Orders': 'Total number of orders placed by customer',\n        'Total_Revenue': 'Total amount spent by customer',\n        'Average_Order_Value': 'Average value per order',\n        'Customer_Lifetime_Value': 'Total revenue from customer',\n        'Repeat_Customer': 'Boolean - whether customer made repeat purchases',\n        'CLV_Segment': 'Customer value segment (Low/Medium/High/VIP)'\n    },\n    'cohort_analysis.csv': {\n        'Cohort_Month': 'Month when customer made first purchase',\n        'Period_Number': 'Number of months after first purchase',\n        'Customers': 'Number of customers active in this period',\n        'Retention_Rate': 'Percentage of cohort still active'\n    },\n    'monthly_performance.csv': {\n        'Month_Year': 'Month-Year period',\n        'Total_Orders': 'Number of orders in month',\n        'Total_Revenue': 'Revenue generated in month',\n        'Unique_Customers': 'Number of unique customers in month',\n        'Completion_Rate': 'Percentage of orders completed'\n    }\n}\n\nfor file, columns in file_descriptions.items():\n    for col, desc in columns.items():\n        data_dictionary['File'].append(file)\n        data_dictionary['Column'].append(col)\n        data_dictionary['Description'].append(desc)\n        data_dictionary['Data_Type'].append('Various')\n\npd.DataFrame(data_dictionary).to_csv('data_dictionary.csv', index=False)\n\nprint(\"\\nüéâ Data processing completed successfully!\")\nprint(\"=\"*50)\nprint(\"üìÅ Generated files for Looker Studio:\")\nfor filename in datasets.keys():\n    print(f\"   ‚Ä¢ {filename}\")\nprint(\"   ‚Ä¢ data_dictionary.csv\")\n\nprint(\"\\nüìä Key Statistics:\")\nprint(f\"   ‚Ä¢ Total Orders: {len(sales_clean):,}\")\nprint(f\"   ‚Ä¢ Total Customers: {sales_clean['Customer Email'].nunique():,}\")\nprint(f\"   ‚Ä¢ Date Range: {sales_clean['Purchase Date'].min().strftime('%Y-%m-%d')} to {sales_clean['Purchase Date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"   ‚Ä¢ Total Revenue: ${sales_clean['Grand Total (Base)'].sum():,.2f}\")\n\nprint(\"\\nüöÄ Ready for Looker Studio import!\")\nprint(\"Upload these CSV files to create your eCommerce analytics dashboard.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.928726Z","iopub.status.idle":"2025-06-12T06:30:44.929021Z","shell.execute_reply.started":"2025-06-12T06:30:44.928902Z","shell.execute_reply":"2025-06-12T06:30:44.928913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. OPTIMIZED DATASETS FOR LOOKER STUDIO PERFORMANCE\nprint(\"\\n‚ö° Step 7: Creating optimized datasets for Looker Studio performance...\")\n\ndef create_time_series_aggregations(sales_df, customer_df):\n    \"\"\"Create time-based aggregated datasets for faster Looker Studio performance\"\"\"\n    \n    # 7.1 YEARLY PERFORMANCE DATASET\n    yearly_performance = sales_df.groupby('Year').agg({\n        'ID': 'count',\n        'Customer Email': 'nunique',\n        'Grand Total (Base)': ['sum', 'mean'],\n        'Discount Amount': 'sum',\n        'Status': lambda x: (x == 'complete').sum()\n    }).round(2)\n    \n    yearly_performance.columns = [\n        'Total_Orders', 'Unique_Customers', 'Total_Revenue', \n        'Average_Order_Value', 'Total_Discounts', 'Completed_Orders'\n    ]\n    \n    # Add YoY growth calculations\n    yearly_performance['Revenue_Growth_Rate'] = yearly_performance['Total_Revenue'].pct_change() * 100\n    yearly_performance['Customer_Growth_Rate'] = yearly_performance['Unique_Customers'].pct_change() * 100\n    yearly_performance['Order_Growth_Rate'] = yearly_performance['Total_Orders'].pct_change() * 100\n    \n    # Calculate retention metrics per year\n    for year in yearly_performance.index:\n        year_customers = sales_df[sales_df['Year'] == year]['Customer Email'].unique()\n        if year > yearly_performance.index.min():\n            prev_year = year - 1\n            prev_customers = sales_df[sales_df['Year'] == prev_year]['Customer Email'].unique()\n            retained_customers = len(set(year_customers) & set(prev_customers))\n            yearly_performance.loc[year, 'Retained_Customers'] = retained_customers\n            yearly_performance.loc[year, 'Retention_Rate'] = (retained_customers / len(prev_customers) * 100) if len(prev_customers) > 0 else 0\n    \n    yearly_performance = yearly_performance.round(2).reset_index()\n    \n    # 7.2 MONTHLY TREND DATASET (Aggregated)\n    monthly_trends = sales_df.groupby(['Year', 'Month']).agg({\n        'ID': 'count',\n        'Customer Email': 'nunique',\n        'Grand Total (Base)': ['sum', 'mean'],\n        'Discount Amount': 'sum'\n    }).round(2)\n    \n    monthly_trends.columns = ['Orders', 'Customers', 'Revenue', 'AOV', 'Discounts']\n    monthly_trends = monthly_trends.reset_index()\n    monthly_trends['Month_Name'] = pd.to_datetime(monthly_trends[['Year', 'Month']].assign(day=1)).dt.strftime('%B')\n    monthly_trends['Year_Month'] = monthly_trends['Year'].astype(str) + '-' + monthly_trends['Month'].astype(str).str.zfill(2)\n    \n    # 7.3 QUARTERLY PERFORMANCE DATASET\n    quarterly_performance = sales_df.groupby(['Year', 'Quarter']).agg({\n        'ID': 'count',\n        'Customer Email': 'nunique',\n        'Grand Total (Base)': ['sum', 'mean'],\n        'Discount Amount': 'sum'\n    }).round(2)\n    \n    quarterly_performance.columns = ['Orders', 'Customers', 'Revenue', 'AOV', 'Discounts']\n    quarterly_performance = quarterly_performance.reset_index()\n    quarterly_performance['Quarter_Label'] = 'Q' + quarterly_performance['Quarter'].astype(str) + ' ' + quarterly_performance['Year'].astype(str)\n    \n    return yearly_performance, monthly_trends, quarterly_performance\n\ndef create_customer_segments_analysis(customer_df):\n    \"\"\"Create customer segment analysis datasets\"\"\"\n    \n    # 7.4 CUSTOMER LIFETIME VALUE SEGMENTS\n    clv_segments = customer_df.groupby('CLV_Segment').agg({\n        'EMAIL': 'count',\n        'Total_Revenue': ['sum', 'mean'],\n        'Total_Orders': ['sum', 'mean'],\n        'Average_Order_Value': 'mean',\n        'Purchase_Frequency_Days': 'mean'\n    }).round(2)\n    \n    clv_segments.columns = [\n        'Customer_Count', 'Total_Segment_Revenue', 'Avg_Customer_Revenue',\n        'Total_Segment_Orders', 'Avg_Customer_Orders', 'Avg_Order_Value', 'Avg_Purchase_Frequency'\n    ]\n    clv_segments = clv_segments.reset_index()\n    \n    # Calculate segment contribution percentages\n    total_revenue = clv_segments['Total_Segment_Revenue'].sum()\n    total_customers = clv_segments['Customer_Count'].sum()\n    clv_segments['Revenue_Contribution_Pct'] = (clv_segments['Total_Segment_Revenue'] / total_revenue * 100).round(2)\n    clv_segments['Customer_Contribution_Pct'] = (clv_segments['Customer_Count'] / total_customers * 100).round(2)\n    \n    # 7.5 REPEAT VS NEW CUSTOMER ANALYSIS\n    repeat_analysis = customer_df.groupby('Repeat_Customer').agg({\n        'EMAIL': 'count',\n        'Total_Revenue': ['sum', 'mean'],\n        'Total_Orders': ['sum', 'mean'],\n        'Average_Order_Value': 'mean'\n    }).round(2)\n    \n    repeat_analysis.columns = [\n        'Customer_Count', 'Total_Revenue', 'Avg_Revenue_Per_Customer',\n        'Total_Orders', 'Avg_Orders_Per_Customer', 'Average_Order_Value'\n    ]\n    repeat_analysis = repeat_analysis.reset_index()\n    repeat_analysis['Customer_Type'] = repeat_analysis['Repeat_Customer'].map({True: 'Repeat Customer', False: 'One-time Customer'})\n    \n    return clv_segments, repeat_analysis\n\ndef create_cohort_summary(cohort_df):\n    \"\"\"Create summarized cohort analysis for dashboard\"\"\"\n    \n    # 7.6 COHORT RETENTION SUMMARY (Monthly cohorts with key periods)\n    key_periods = [0, 1, 3, 6, 12]  # First purchase, 1 month, 3 months, 6 months, 1 year\n    cohort_summary = cohort_df[cohort_df['Period_Number'].isin(key_periods)].copy()\n    \n    # Pivot for easier visualization\n    cohort_pivot = cohort_summary.pivot(index='Cohort_Month', columns='Period_Number', values='Retention_Rate').reset_index()\n    cohort_pivot.columns = ['Cohort_Month'] + [f'Month_{int(col)}' if col != 'Cohort_Month' else col for col in cohort_pivot.columns[1:]]\n    \n    # 7.7 COHORT SIZE AND ACQUISITION TRENDS\n    cohort_acquisition = cohort_df[cohort_df['Period_Number'] == 0][['Cohort_Month', 'Cohort_Size']].copy()\n    cohort_acquisition['Cohort_Month_Str'] = cohort_acquisition['Cohort_Month'].astype(str)\n    cohort_acquisition['Year'] = cohort_acquisition['Cohort_Month'].dt.year\n    cohort_acquisition['Month'] = cohort_acquisition['Cohort_Month'].dt.month\n    \n    return cohort_pivot, cohort_acquisition\n\ndef create_email_subscriber_analysis(retention_df):\n    \"\"\"Create email subscriber analysis datasets\"\"\"\n    \n    # 7.8 EMAIL SUBSCRIBER TRENDS\n    newsletter_analysis = retention_df.groupby('NEWSLETTER_FREQUENCY').agg({\n        'EMAIL': 'count',\n        'TOTAL_ORDER_AMOUNT': ['sum', 'mean', 'count']\n    }).round(2)\n    \n    newsletter_analysis.columns = ['Subscriber_Count', 'Total_Revenue', 'Avg_Revenue_Per_Subscriber', 'Active_Buyers']\n    newsletter_analysis = newsletter_analysis.reset_index()\n    newsletter_analysis['Conversion_Rate'] = (newsletter_analysis['Active_Buyers'] / newsletter_analysis['Subscriber_Count'] * 100).round(2)\n    \n    # 7.9 DEMOGRAPHIC ANALYSIS\n    demographic_analysis = retention_df.groupby('GENDER').agg({\n        'EMAIL': 'count',\n        'TOTAL_ORDER_AMOUNT': ['sum', 'mean']\n    }).round(2)\n    \n    demographic_analysis.columns = ['Customer_Count', 'Total_Revenue', 'Avg_Revenue_Per_Customer']\n    demographic_analysis = demographic_analysis.reset_index()\n    \n    # Regional analysis\n    regional_analysis = retention_df.groupby('REGION').agg({\n        'EMAIL': 'count',\n        'TOTAL_ORDER_AMOUNT': ['sum', 'mean']\n    }).round(2)\n    \n    regional_analysis.columns = ['Customer_Count', 'Total_Revenue', 'Avg_Revenue_Per_Customer']\n    regional_analysis = regional_analysis.reset_index()\n    regional_analysis = regional_analysis.sort_values('Total_Revenue', ascending=False).head(20)  # Top 20 regions\n    \n    return newsletter_analysis, demographic_analysis, regional_analysis\n\ndef create_dashboard_kpis(sales_df, customer_df):\n    \"\"\"Create KPI summary datasets for different time periods\"\"\"\n    \n    # 7.10 MONTHLY KPI TRENDS\n    monthly_kpis = []\n    \n    for month_year in sales_df['Month_Year'].unique():\n        month_data = sales_df[sales_df['Month_Year'] == month_year]\n        month_customers = customer_df[customer_df['EMAIL'].isin(month_data['Customer Email'].unique())]\n        \n        total_customers = month_data['Customer Email'].nunique()\n        repeat_customers = len(month_customers[month_customers['Repeat_Customer']])\n        \n        kpi_row = {\n            'Month_Year': month_year,\n            'Total_Orders': len(month_data),\n            'Total_Revenue': month_data['Grand Total (Base)'].sum(),\n            'Total_Customers': total_customers,\n            'New_Customers': total_customers - repeat_customers,\n            'Repeat_Customers': repeat_customers,\n            'AOV': month_data['Grand Total (Base)'].mean(),\n            'Retention_Rate': (repeat_customers / total_customers * 100) if total_customers > 0 else 0\n        }\n        monthly_kpis.append(kpi_row)\n    \n    monthly_kpis_df = pd.DataFrame(monthly_kpis).round(2)\n    monthly_kpis_df = monthly_kpis_df.sort_values('Month_Year')\n    \n    # 7.11 YEARLY KPI SUMMARY\n    yearly_kpis = []\n    \n    for year in sales_df['Year'].unique():\n        year_data = sales_df[sales_df['Year'] == year]\n        year_customers = customer_df[customer_df['EMAIL'].isin(year_data['Customer Email'].unique())]\n        \n        total_customers = year_data['Customer Email'].nunique()\n        repeat_customers = len(year_customers[year_customers['Repeat_Customer']])\n        \n        kpi_row = {\n            'Year': year,\n            'Total_Orders': len(year_data),\n            'Total_Revenue': year_data['Grand Total (Base)'].sum(),\n            'Total_Customers': total_customers,\n            'New_Customers': total_customers - repeat_customers,\n            'Repeat_Customers': repeat_customers,\n            'AOV': year_data['Grand Total (Base)'].mean(),\n            'Retention_Rate': (repeat_customers / total_customers * 100) if total_customers > 0 else 0\n        }\n        yearly_kpis.append(kpi_row)\n    \n    yearly_kpis_df = pd.DataFrame(yearly_kpis).round(2)\n    yearly_kpis_df = yearly_kpis_df.sort_values('Year')\n    \n    return monthly_kpis_df, yearly_kpis_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.930650Z","iopub.status.idle":"2025-06-12T06:30:44.931089Z","shell.execute_reply.started":"2025-06-12T06:30:44.930857Z","shell.execute_reply":"2025-06-12T06:30:44.930874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate all optimized datasets\nprint(\"üîÑ Creating yearly performance dataset...\")\nyearly_perf, monthly_trends, quarterly_perf = create_time_series_aggregations(sales_clean, customer_analytics)\n\nprint(f\"Shape: {yearly_perf.shape}\")\nyearly_perf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.932900Z","iopub.status.idle":"2025-06-12T06:30:44.933307Z","shell.execute_reply.started":"2025-06-12T06:30:44.933119Z","shell.execute_reply":"2025-06-12T06:30:44.933137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {monthly_trends.shape}\")\n\nmonthly_trends.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.934367Z","iopub.status.idle":"2025-06-12T06:30:44.934851Z","shell.execute_reply.started":"2025-06-12T06:30:44.934616Z","shell.execute_reply":"2025-06-12T06:30:44.934657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {quarterly_perf.shape}\")\nquarterly_perf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.936191Z","iopub.status.idle":"2025-06-12T06:30:44.936480Z","shell.execute_reply.started":"2025-06-12T06:30:44.936355Z","shell.execute_reply":"2025-06-12T06:30:44.936367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Creating customer segment analysis...\")\nclv_segments, repeat_analysis = create_customer_segments_analysis(customer_analytics)\n\nprint(f\"Shape: {clv_segments.shape}\")\nclv_segments.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.938131Z","iopub.status.idle":"2025-06-12T06:30:44.938501Z","shell.execute_reply.started":"2025-06-12T06:30:44.938369Z","shell.execute_reply":"2025-06-12T06:30:44.938381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {repeat_analysis.shape}\")\nrepeat_analysis.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.940076Z","iopub.status.idle":"2025-06-12T06:30:44.940436Z","shell.execute_reply.started":"2025-06-12T06:30:44.940252Z","shell.execute_reply":"2025-06-12T06:30:44.940263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Creating cohort summaries...\")\ncohort_pivot, cohort_acquisition = create_cohort_summary(cohort_analysis)\n\nprint(f\"Shape: {cohort_pivot.shape}\")\ncohort_pivot.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.941765Z","iopub.status.idle":"2025-06-12T06:30:44.942024Z","shell.execute_reply.started":"2025-06-12T06:30:44.941913Z","shell.execute_reply":"2025-06-12T06:30:44.941923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {cohort_acquisition.shape}\")\ncohort_acquisition.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.943349Z","iopub.status.idle":"2025-06-12T06:30:44.943769Z","shell.execute_reply.started":"2025-06-12T06:30:44.943562Z","shell.execute_reply":"2025-06-12T06:30:44.943579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Creating email subscriber analysis...\")\nnewsletter_analysis, demographic_analysis, regional_analysis = create_email_subscriber_analysis(retention_clean)\n\nprint(f\"Shape: {newsletter_analysis.shape}\")\nnewsletter_analysis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.945718Z","iopub.status.idle":"2025-06-12T06:30:44.946224Z","shell.execute_reply.started":"2025-06-12T06:30:44.945901Z","shell.execute_reply":"2025-06-12T06:30:44.945914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {demographic_analysis.shape}\")\ndemographic_analysis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.947714Z","iopub.status.idle":"2025-06-12T06:30:44.947993Z","shell.execute_reply.started":"2025-06-12T06:30:44.947871Z","shell.execute_reply":"2025-06-12T06:30:44.947883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {regional_analysis.shape}\")\nregional_analysis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.950027Z","iopub.status.idle":"2025-06-12T06:30:44.950303Z","shell.execute_reply.started":"2025-06-12T06:30:44.950184Z","shell.execute_reply":"2025-06-12T06:30:44.950194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"üîÑ Creating dashboard KPIs...\")\nmonthly_kpis, yearly_kpis = create_dashboard_kpis(sales_clean, customer_analytics)\n\nprint(f\"Shape: {monthly_kpis.shape}\")\nmonthly_kpis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.951544Z","iopub.status.idle":"2025-06-12T06:30:44.951996Z","shell.execute_reply.started":"2025-06-12T06:30:44.951805Z","shell.execute_reply":"2025-06-12T06:30:44.951824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape: {yearly_kpis.shape}\")\nyearly_kpis.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.953028Z","iopub.status.idle":"2025-06-12T06:30:44.953349Z","shell.execute_reply.started":"2025-06-12T06:30:44.953221Z","shell.execute_reply":"2025-06-12T06:30:44.953233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. SAMPLE DATASETS FOR TESTING\nprint(\"\\nüß™ Creating sample datasets for testing...\")\n\n# Create smaller sample datasets for initial dashboard development\nsample_datasets = {\n    'sample_sales_data.csv': sales_clean.sample(n=min(10000, len(sales_clean))),\n    'sample_customer_analytics.csv': customer_analytics.sample(n=min(5000, len(customer_analytics))),\n    'sample_monthly_trends.csv': monthly_trends\n}\n\nfor filename, dataset in sample_datasets.items():\n    dataset.to_csv(filename, index=False)\n    print(f\"‚úÖ Saved: {filename} ({dataset.shape})\")\n\n# 9. GENERATE COMPREHENSIVE DATA DICTIONARY\nprint(\"\\nüìã Step 8: Generating comprehensive data dictionary...\")\n\ndata_dictionary = {\n    'File': [],\n    'Column': [],\n    'Description': [],\n    'Data_Type': [],\n    'Use_Case': []\n}\n\n# Comprehensive file descriptions\nfile_descriptions = {\n    'yearly_performance.csv': {\n        'Year': ('Year', 'int', 'Year-over-year analysis, growth trends'),\n        'Total_Orders': ('Total number of orders in year', 'int', 'Annual performance tracking'),\n        'Total_Revenue': ('Total revenue generated in year', 'float', 'Revenue trend analysis'),\n        'Revenue_Growth_Rate': ('Year-over-year revenue growth percentage', 'float', 'Growth rate visualization'),\n        'Retention_Rate': ('Customer retention rate for the year', 'float', 'Retention trend analysis')\n    },\n    'monthly_trends.csv': {\n        'Year': ('Year', 'int', 'Monthly trend analysis'),\n        'Month': ('Month number (1-12)', 'int', 'Monthly trend analysis'),\n        'Orders': ('Number of orders in month', 'int', 'Monthly performance tracking'),\n        'Revenue': ('Total revenue in month', 'float', 'Monthly revenue trends'),\n        'Customers': ('Unique customers in month', 'int', 'Customer acquisition trends')\n    },\n    'customer_segments_clv.csv': {\n        'CLV_Segment': ('Customer lifetime value segment', 'str', 'Customer segmentation analysis'),\n        'Customer_Count': ('Number of customers in segment', 'int', 'Segment size analysis'),\n        'Total_Segment_Revenue': ('Total revenue from segment', 'float', 'Segment contribution analysis'),\n        'Revenue_Contribution_Pct': ('Percentage of total revenue from segment', 'float', 'Segment importance metrics')\n    },\n    'cohort_retention_summary.csv': {\n        'Cohort_Month': ('Month when customers made first purchase', 'str', 'Cohort analysis'),\n        'Month_0': ('Retention rate at first purchase (100%)', 'float', 'Cohort retention tracking'),\n        'Month_1': ('Retention rate after 1 month', 'float', 'Cohort retention tracking'),\n        'Month_3': ('Retention rate after 3 months', 'float', 'Cohort retention tracking'),\n        'Month_6': ('Retention rate after 6 months', 'float', 'Cohort retention tracking'),\n        'Month_12': ('Retention rate after 12 months', 'float', 'Cohort retention tracking')\n    },\n    'monthly_kpi_trends.csv': {\n        'Month_Year': ('Month-Year period', 'str', 'KPI trend analysis'),\n        'Total_Orders': ('Orders in month', 'int', 'Performance monitoring'),\n        'Total_Revenue': ('Revenue in month', 'float', 'Revenue tracking'),\n        'Retention_Rate': ('Customer retention rate for month', 'float', 'Retention monitoring'),\n        'AOV': ('Average order value for month', 'float', 'Order value trends')\n    }\n}\n\nfor file, columns in file_descriptions.items():\n    for col, (desc, dtype, use_case) in columns.items():\n        data_dictionary['File'].append(file)\n        data_dictionary['Column'].append(col)\n        data_dictionary['Description'].append(desc)\n        data_dictionary['Data_Type'].append(dtype)\n        data_dictionary['Use_Case'].append(use_case)\n\npd.DataFrame(data_dictionary).to_csv('comprehensive_data_dictionary.csv', index=False)\n\nprint(\"\\nüéâ Optimized data processing completed successfully!\")\nprint(\"=\"*50)\nprint(\"üìÅ OPTIMIZED FILES FOR LOOKER STUDIO (Small & Fast):\")\nfor filename in optimized_datasets.keys():\n    print(f\"   ‚Ä¢ {filename}\")\n\nprint(\"\\nüìÅ SAMPLE FILES FOR TESTING:\")\nfor filename in sample_datasets.keys():\n    print(f\"   ‚Ä¢ {filename}\")\n\nprint(\"\\nüìÅ ORIGINAL COMPLETE FILES (Backup):\")\nfor filename in datasets.keys():\n    print(f\"   ‚Ä¢ {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.954315Z","iopub.status.idle":"2025-06-12T06:30:44.954581Z","shell.execute_reply.started":"2025-06-12T06:30:44.954455Z","shell.execute_reply":"2025-06-12T06:30:44.954466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save optimized datasets\noptimized_datasets = {\n    # Time-based analysis\n    'yearly_performance.csv': yearly_perf,\n    'monthly_trends.csv': monthly_trends,\n    'quarterly_performance.csv': quarterly_perf,\n    \n    # Customer analysis\n    'customer_segments_clv.csv': clv_segments,\n    'repeat_vs_new_customers.csv': repeat_analysis,\n    \n    # Cohort analysis\n    'cohort_retention_summary.csv': cohort_pivot,\n    'cohort_acquisition_trends.csv': cohort_acquisition,\n    \n    # Email & Demographics\n    'newsletter_subscriber_analysis.csv': newsletter_analysis,\n    'demographic_analysis.csv': demographic_analysis,\n    'regional_performance.csv': regional_analysis,\n    \n    # KPI Dashboards\n    'monthly_kpi_trends.csv': monthly_kpis,\n    'yearly_kpi_summary.csv': yearly_kpis\n}\n\nprint(\"\\nüíæ Saving optimized datasets for Looker Studio...\")\nfor filename, dataset in optimized_datasets.items():\n    dataset.to_csv(filename, index=False)\n    print(f\"‚úÖ Saved: {filename} ({dataset.shape})\")\n\n\nprint(\"\\nüíæ Save done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.956493Z","iopub.status.idle":"2025-06-12T06:30:44.956905Z","shell.execute_reply.started":"2025-06-12T06:30:44.956728Z","shell.execute_reply":"2025-06-12T06:30:44.956742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nüìä DASHBOARD RECOMMENDATIONS:\")\nprint(\"   üìà Use 'yearly_performance.csv' for year-over-year analysis\")\nprint(\"   üìÖ Use 'monthly_trends.csv' for monthly trend charts\")\nprint(\"   üë• Use 'customer_segments_clv.csv' for customer segmentation\")\nprint(\"   üîÑ Use 'cohort_retention_summary.csv' for cohort analysis\")\nprint(\"   üìß Use 'newsletter_subscriber_analysis.csv' for email metrics\")\nprint(\"   üìã Use 'monthly_kpi_trends.csv' for KPI dashboards\")\n\nprint(f\"\\nüìä Key Statistics:\")\nprint(f\"   ‚Ä¢ Total Orders: {len(sales_clean):,}\")\nprint(f\"   ‚Ä¢ Total Customers: {sales_clean['Customer Email'].nunique():,}\")\nprint(f\"   ‚Ä¢ Date Range: {sales_clean['Purchase Date'].min().strftime('%Y-%m-%d')} to {sales_clean['Purchase Date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"   ‚Ä¢ Total Revenue: ${sales_clean['Grand Total (Base)'].sum():,.2f}\")\n\nprint(\"\\nüöÄ READY FOR LOOKER STUDIO!\")\nprint(\"Start with the optimized datasets for better dashboard performance!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.958316Z","iopub.status.idle":"2025-06-12T06:30:44.958602Z","shell.execute_reply.started":"2025-06-12T06:30:44.958465Z","shell.execute_reply":"2025-06-12T06:30:44.958476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to summarize each DataFrame\ndef summarize_df(df, name):\n    print(f\"\\nüìÑ SUMMARY FOR: {name}\\n\")\n    print(\"\\n Info:\")\n    print(df.info())\n    print(\"First 5 Rows:\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.960465Z","iopub.status.idle":"2025-06-12T06:30:44.960793Z","shell.execute_reply.started":"2025-06-12T06:30:44.960655Z","shell.execute_reply":"2025-06-12T06:30:44.960671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(yearly_perf, \"yearly_performance\")\nyearly_perf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.962030Z","iopub.status.idle":"2025-06-12T06:30:44.962434Z","shell.execute_reply.started":"2025-06-12T06:30:44.962232Z","shell.execute_reply":"2025-06-12T06:30:44.962251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(monthly_trends, \"monthly_trends\")\nmonthly_trends.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.963331Z","iopub.status.idle":"2025-06-12T06:30:44.963828Z","shell.execute_reply.started":"2025-06-12T06:30:44.963593Z","shell.execute_reply":"2025-06-12T06:30:44.963612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(clv_segments, \"clv_segments\")\nclv_segments.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.967905Z","iopub.status.idle":"2025-06-12T06:30:44.968354Z","shell.execute_reply.started":"2025-06-12T06:30:44.968135Z","shell.execute_reply":"2025-06-12T06:30:44.968152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(cohort_pivot, \"cohort_pivot\")\ncohort_pivot.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.970085Z","iopub.status.idle":"2025-06-12T06:30:44.970497Z","shell.execute_reply.started":"2025-06-12T06:30:44.970304Z","shell.execute_reply":"2025-06-12T06:30:44.970330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(newsletter_analysis, \"newsletter_analysis\")\nnewsletter_analysis.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.971410Z","iopub.status.idle":"2025-06-12T06:30:44.971875Z","shell.execute_reply.started":"2025-06-12T06:30:44.971694Z","shell.execute_reply":"2025-06-12T06:30:44.971711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_df(monthly_kpis, \"monthly_kpis\")\nmonthly_kpis.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.973065Z","iopub.status.idle":"2025-06-12T06:30:44.973663Z","shell.execute_reply.started":"2025-06-12T06:30:44.973421Z","shell.execute_reply":"2025-06-12T06:30:44.973446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finding Monthly top 10 SKUs ","metadata":{}},{"cell_type":"code","source":"# Read the CSV files (update paths as needed)\nfile1 = \"/kaggle/input/crm-data-analysis/mehedi-export.csv\"\n\n# Load the data into DataFrames\nexport_df = pd.read_csv(file1, encoding='utf-8', low_memory=False)\n\nprint(f\"‚úÖ Dataset shape: {export_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.975974Z","iopub.status.idle":"2025-06-12T06:30:44.976616Z","shell.execute_reply.started":"2025-06-12T06:30:44.976400Z","shell.execute_reply":"2025-06-12T06:30:44.976419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"export_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.978729Z","iopub.status.idle":"2025-06-12T06:30:44.979052Z","shell.execute_reply.started":"2025-06-12T06:30:44.978913Z","shell.execute_reply":"2025-06-12T06:30:44.978929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"export_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.980869Z","iopub.status.idle":"2025-06-12T06:30:44.981810Z","shell.execute_reply.started":"2025-06-12T06:30:44.981099Z","shell.execute_reply":"2025-06-12T06:30:44.981267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Load the data\nfile_path = \"/kaggle/input/crm-data-analysis/mehedi-export.csv\"\nexport_df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n\n# Step 2: Convert 'created_at' to datetime and extract 'YearMonth'\nexport_df['created_at'] = pd.to_datetime(export_df['created_at'], errors='coerce')\nexport_df['YearMonth'] = export_df['created_at'].dt.to_period('M').astype(str)\n\n# Step 3: Drop unnecessary columns\ncolumns_to_keep = ['customer_email', 'created_at', 'YearMonth', 'sku_list']\nexport_df = export_df[columns_to_keep]\n\n# Step 4: Clean sku_list and explode rows\nexport_df['sku_list'] = export_df['sku_list'].str.replace(r'\\s+', '', regex=True)\nexport_df['sku_list'] = export_df['sku_list'].str.split(',')\ndf_exploded = export_df.explode('sku_list')\n\n# Remove any empty or NaN SKUs after splitting\ndf_exploded = df_exploded[df_exploded['sku_list'].notna() & (df_exploded['sku_list'] != '')]\n\n# Step 5: Group by YearMonth and SKU to count purchases\nsku_counts = (\n    df_exploded\n    .groupby(['YearMonth', 'sku_list'])\n    .size()\n    .reset_index(name='purchase_count')\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.983273Z","iopub.status.idle":"2025-06-12T06:30:44.983770Z","shell.execute_reply.started":"2025-06-12T06:30:44.983568Z","shell.execute_reply":"2025-06-12T06:30:44.983584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"export_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.985584Z","iopub.status.idle":"2025-06-12T06:30:44.986020Z","shell.execute_reply.started":"2025-06-12T06:30:44.985826Z","shell.execute_reply":"2025-06-12T06:30:44.985844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sku_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.988219Z","iopub.status.idle":"2025-06-12T06:30:44.988662Z","shell.execute_reply.started":"2025-06-12T06:30:44.988439Z","shell.execute_reply":"2025-06-12T06:30:44.988456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Get Top 10 SKUs per month\ntop10_skus_monthly = (\n    sku_counts\n    .sort_values(['YearMonth', 'purchase_count'], ascending=[True, False])\n    .groupby('YearMonth')\n    .head(10)\n    .reset_index(drop=True)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.989868Z","iopub.status.idle":"2025-06-12T06:30:44.990388Z","shell.execute_reply.started":"2025-06-12T06:30:44.990076Z","shell.execute_reply":"2025-06-12T06:30:44.990092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: Save the result to CSV for Looker Studio upload\noutput_file = \"top10_skus_monthly.csv\"\ntop10_skus_monthly.to_csv(output_file, index=False)\n\nprint(f\"‚úÖ Top 10 SKUs per month saved to: {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.992002Z","iopub.status.idle":"2025-06-12T06:30:44.992383Z","shell.execute_reply.started":"2025-06-12T06:30:44.992210Z","shell.execute_reply":"2025-06-12T06:30:44.992228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"export_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.993050Z","iopub.status.idle":"2025-06-12T06:30:44.993329Z","shell.execute_reply.started":"2025-06-12T06:30:44.993210Z","shell.execute_reply":"2025-06-12T06:30:44.993222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sorted list of unique SKUs\nunique_skus_sorted = sorted(df_exploded['sku_list'].dropna().unique())\nprint(unique_skus_sorted[:6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.995025Z","iopub.status.idle":"2025-06-12T06:30:44.995587Z","shell.execute_reply.started":"2025-06-12T06:30:44.995301Z","shell.execute_reply":"2025-06-12T06:30:44.995368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure all SKUs are strings\ndf_exploded['sku_str'] = df_exploded['sku_list'].astype(str)\n\n# Filter rows where SKU is NOT a number\nnon_numeric_skus = df_exploded[~df_exploded['sku_str'].str.isdigit()]\n\n# Show unique non-numeric SKUs\nprint(non_numeric_skus['sku_list'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.996604Z","iopub.status.idle":"2025-06-12T06:30:44.996979Z","shell.execute_reply.started":"2025-06-12T06:30:44.996836Z","shell.execute_reply":"2025-06-12T06:30:44.996854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"non_numeric_skus = [sku for sku in unique_skus_sorted if not str(sku).isdigit()]\nprint(non_numeric_skus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:44.998979Z","iopub.status.idle":"2025-06-12T06:30:44.999241Z","shell.execute_reply.started":"2025-06-12T06:30:44.999130Z","shell.execute_reply":"2025-06-12T06:30:44.999141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Top 15 SKUs per year","metadata":{}},{"cell_type":"code","source":"# # Step 1: Load the data\n# file_path = \"/kaggle/input/crm-data-analysis/mehedi-export.csv\"\n# export_df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n\n# export_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.001033Z","iopub.status.idle":"2025-06-12T06:30:45.001447Z","shell.execute_reply.started":"2025-06-12T06:30:45.001253Z","shell.execute_reply":"2025-06-12T06:30:45.001272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 2: Convert 'created_at' to datetime and extract 'Year'\n# export_df['created_at'] = pd.to_datetime(export_df['created_at'], errors='coerce')\n# export_df['Year'] = export_df['created_at'].dt.year\n\n# # Step 3: Keep only necessary columns\n# columns_to_keep = ['customer_email', 'created_at', 'Year', 'sku_list']\n# export_df = export_df[columns_to_keep]\n\n# # Step 4: Clean and explode SKU list\n# export_df['sku_list'] = export_df['sku_list'].astype(str).str.replace(r'\\s+', '', regex=True)\n# export_df['sku_list'] = export_df['sku_list'].str.split(',')\n# df_exploded = export_df.explode('sku_list')\n\n# # Filter out empty or NaN SKUs\n# df_exploded = df_exploded[df_exploded['sku_list'].notna() & (df_exploded['sku_list'] != '')]\n\n# # ‚úÖ Filter only numeric SKUs\n# df_exploded = df_exploded[df_exploded['sku_list'].str.isdigit()]\n\n# # Step 5: Count purchases per SKU per year\n# sku_counts_yearly = (\n#     df_exploded\n#     .groupby(['Year', 'sku_list'])\n#     .size()\n#     .reset_index(name='purchase_count')\n# )\n\n# # Step 6: Get Top 15 SKUs per year\n# top15_skus_yearly = (\n#     sku_counts_yearly\n#     .sort_values(['Year', 'purchase_count'], ascending=[True, False])\n#     .groupby('Year')\n#     .head(15)\n#     .reset_index(drop=True)\n# )\n\n# # Step 7: Get repurchase rate for each SKU in top15_skus_yearly\n# # Count purchases per customer per SKU per year\n# sku_customer_counts = (\n#     df_exploded\n#     .groupby(['Year', 'sku_list', 'customer_email'])\n#     .size()\n#     .reset_index(name='purchase_count')\n# )\n\n# # Add repurchase flag\n# sku_customer_counts['repurchased'] = sku_customer_counts['purchase_count'] >= 2\n\n# # Filter to only SKUs in the top 15 for that year\n# top_skus_set = set(zip(top15_skus_yearly['Year'], top15_skus_yearly['sku_list']))\n# sku_customer_counts = sku_customer_counts[\n#     sku_customer_counts.apply(lambda row: (row['Year'], row['sku_list']) in top_skus_set, axis=1)\n# ]\n\n# # Compute repurchase rate (as percentage)\n# repurchase_rate = (\n#     sku_customer_counts\n#     .groupby(['Year', 'sku_list'])\n#     .agg(\n#         total_customers=('customer_email', 'nunique'),\n#         repurchasers=('repurchased', 'sum')\n#     )\n#     .reset_index()\n# )\n\n# repurchase_rate['repurchase_rate'] = (\n#     (repurchase_rate['repurchasers'] / repurchase_rate['total_customers']) * 100\n# ).round(2)\n\n# # Step 8: Merge top 15 SKUs with their repurchase rate\n# final_result = pd.merge(top15_skus_yearly, repurchase_rate, on=['Year', 'sku_list'])\n\n# # Final output preview\n# print(\"\\n‚úÖ Top 15 SKUs with Repurchase Rate per Year:\")\n# final_result.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.002380Z","iopub.status.idle":"2025-06-12T06:30:45.002798Z","shell.execute_reply.started":"2025-06-12T06:30:45.002584Z","shell.execute_reply":"2025-06-12T06:30:45.002602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# final_result['sku_list']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.005014Z","iopub.status.idle":"2025-06-12T06:30:45.005477Z","shell.execute_reply.started":"2025-06-12T06:30:45.005222Z","shell.execute_reply":"2025-06-12T06:30:45.005240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 9: Save to CSV\n# final_result.to_csv(\"top15_skus_repurchase_rate_yearly.csv\", index=False)\n# print(\"\\nüìÅ File 'top15_skus_repurchase_rate_yearly.csv' saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.006853Z","iopub.status.idle":"2025-06-12T06:30:45.007197Z","shell.execute_reply.started":"2025-06-12T06:30:45.007067Z","shell.execute_reply":"2025-06-12T06:30:45.007080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Product Catalog with SKU ID (Yearly 2023-2025) (Not fiscal)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load both CSV files\npurchase_data_path = \"/kaggle/input/crm-data-analysis/mehedi-export.csv\"\nproduct_catalog_path = \"/kaggle/input/crm-data-analysis/product catalog - Sheet1.csv\"\n\npurchase_df = pd.read_csv(purchase_data_path, encoding='utf-8', low_memory=False)\n\nprint(purchase_df.info())\npurchase_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:43:40.740744Z","iopub.execute_input":"2025-06-12T06:43:40.741019Z","iopub.status.idle":"2025-06-12T06:43:49.079083Z","shell.execute_reply.started":"2025-06-12T06:43:40.741000Z","shell.execute_reply":"2025-06-12T06:43:49.078086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"product_df = pd.read_csv(product_catalog_path, encoding='utf-8', low_memory=False)\nproduct_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:42:13.381341Z","iopub.execute_input":"2025-06-12T06:42:13.381652Z","iopub.status.idle":"2025-06-12T06:42:13.410451Z","shell.execute_reply.started":"2025-06-12T06:42:13.381611Z","shell.execute_reply":"2025-06-12T06:42:13.409520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"product_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:22:58.328210Z","iopub.execute_input":"2025-06-12T07:22:58.328681Z","iopub.status.idle":"2025-06-12T07:22:58.346979Z","shell.execute_reply.started":"2025-06-12T07:22:58.328650Z","shell.execute_reply":"2025-06-12T07:22:58.345980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Clean Product Catalog ---\nproduct_df.columns = product_df.columns.str.strip()\nproduct_df = product_df.rename(columns={\n    'Model Name - !!!DO NOT CHANGE THE ORDER!!!': 'Model_Name',\n    'SKU': 'SKU',\n    'Typ I': 'Type_1'\n})\n\n# Remove accessories from 'Type_1'\nproduct_df['Type_1'] = product_df['Type_1'].astype(str).str.strip()\nproduct_df = product_df[product_df['Type_1'].str.lower() != 'accessories']\n\n# Clean Model_Name and SKU\nproduct_df['Model_Name'] = product_df['Model_Name'].astype(str).str.strip()\nproduct_df['SKU'] = product_df['SKU'].astype(str).str.strip()\n\n# --- Clean and Prepare Purchase Data ---\npurchase_df['created_at'] = pd.to_datetime(purchase_df['created_at'], errors='coerce')\npurchase_df['Year'] = purchase_df['created_at'].dt.year\n\ncolumns_to_keep = ['customer_email', 'created_at', 'Year', 'sku_list']\npurchase_df = purchase_df[columns_to_keep]\n\npurchase_df['sku_list'] = purchase_df['sku_list'].astype(str).str.replace(r'\\s+', '', regex=True)\npurchase_df['sku_list'] = purchase_df['sku_list'].str.split(',')\n\ndf_exploded = purchase_df.explode('sku_list')\n\n# Remove empty or NaN SKUs\ndf_exploded = df_exploded[df_exploded['sku_list'].notna() & (df_exploded['sku_list'] != '')]\n\n# --- Merge with Product Catalog ---\n# Convert SKU column to string before merge\ndf_exploded['sku_list'] = df_exploded['sku_list'].astype(str)\nmerged_df = pd.merge(df_exploded, product_df, how='inner', left_on='sku_list', right_on='SKU')\n\n# ‚úÖ Filter only numerical SKU IDs after merge\nmerged_df = merged_df[merged_df['sku_list'].str.isdigit()]\n\nmerged_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:42:13.411413Z","iopub.execute_input":"2025-06-12T06:42:13.411782Z","iopub.status.idle":"2025-06-12T06:42:18.722150Z","shell.execute_reply.started":"2025-06-12T06:42:13.411747Z","shell.execute_reply":"2025-06-12T06:42:18.721079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:50:50.518231Z","iopub.execute_input":"2025-06-12T06:50:50.518645Z","iopub.status.idle":"2025-06-12T06:50:50.993098Z","shell.execute_reply.started":"2025-06-12T06:50:50.518598Z","shell.execute_reply":"2025-06-12T06:50:50.992114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df['sku_list'].info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:45:34.415457Z","iopub.execute_input":"2025-06-12T07:45:34.415810Z","iopub.status.idle":"2025-06-12T07:45:34.660853Z","shell.execute_reply.started":"2025-06-12T07:45:34.415778Z","shell.execute_reply":"2025-06-12T07:45:34.659925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Count purchases per SKU per year\nsku_counts_yearly = (\n    merged_df\n    .groupby(['Year', 'sku_list'])\n    .size()\n    .reset_index(name='purchase_count')\n)\n\n# Step 2: Top 15 SKUs per year\ntop15_skus_yearly = (\n    sku_counts_yearly\n    .sort_values(['Year', 'purchase_count'], ascending=[True, False])\n    .groupby('Year')\n    .head(15)\n    .reset_index(drop=True)\n)\n\n# Step 3: Repurchase Calculation\nsku_customer_counts = (\n    merged_df\n    .groupby(['Year', 'sku_list', 'customer_email'])\n    .size()\n    .reset_index(name='purchase_count')\n)\n\nsku_customer_counts['repurchased'] = sku_customer_counts['purchase_count'] >= 2\n\n# Filter only SKUs in top 15\ntop_skus_set = set(zip(top15_skus_yearly['Year'], top15_skus_yearly['sku_list']))\nsku_customer_counts = sku_customer_counts[\n    sku_customer_counts.apply(lambda row: (row['Year'], row['sku_list']) in top_skus_set, axis=1)\n]\n\n# Compute repurchase rate\nrepurchase_df = (\n    sku_customer_counts\n    .groupby(['Year', 'sku_list'])\n    .agg(\n        total_customers=('customer_email', 'nunique'),\n        repurchasers=('repurchased', 'sum')\n    )\n    .reset_index()\n)\n\nrepurchase_df['repurchase_rate'] = (\n    (repurchase_df['repurchasers'] / repurchase_df['total_customers']) * 100\n).round(2)\n\n# Step 4: Final Merge for Report\nfinal_df = pd.merge(top15_skus_yearly, repurchase_df, on=['Year', 'sku_list'])\n\n# Merge with product info (Model Name, Type_1)\nfinal_df = pd.merge(final_df, product_df, how='left', left_on='sku_list', right_on='SKU')\n\n# Final columns\nfinal_df = final_df[['Year', 'Model_Name', 'SKU', 'Type_1', 'purchase_count', 'total_customers', 'repurchasers', 'repurchase_rate']]\n\n\nfinal_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:45:34.677318Z","iopub.execute_input":"2025-06-12T07:45:34.677762Z","iopub.status.idle":"2025-06-12T07:45:45.293108Z","shell.execute_reply.started":"2025-06-12T07:45:34.677725Z","shell.execute_reply":"2025-06-12T07:45:45.292099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save to CSV\nfinal_df.to_csv(\"Top15_SKUs_model_repurchase\", index=False)\nprint(\"‚úÖ File saved as 'Top15_SKUs_model_repurchase'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:45:45.294524Z","iopub.execute_input":"2025-06-12T07:45:45.294871Z","iopub.status.idle":"2025-06-12T07:45:45.307900Z","shell.execute_reply.started":"2025-06-12T07:45:45.294842Z","shell.execute_reply":"2025-06-12T07:45:45.306756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Product Catalog with SKU ID (Yearly April 2023 - March 2025) (Fiscal)","metadata":{}},{"cell_type":"code","source":"purchase_df = pd.read_csv(purchase_data_path, encoding='utf-8', low_memory=False)\nproduct_df = pd.read_csv(product_catalog_path, encoding='utf-8', low_memory=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:46:03.303430Z","iopub.execute_input":"2025-06-12T07:46:03.303789Z","iopub.status.idle":"2025-06-12T07:46:11.502898Z","shell.execute_reply.started":"2025-06-12T07:46:03.303765Z","shell.execute_reply":"2025-06-12T07:46:11.501714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, ensure created_at is in datetime format (you've already done this in your script)\npurchase_df['created_at'] = pd.to_datetime(purchase_df['created_at'], errors='coerce')\n\n# Get the earliest and latest date\nmin_date = purchase_df['created_at'].min()\nmax_date = purchase_df['created_at'].max()\n\n# Summary of the created_at column\nsummary = purchase_df['created_at'].describe()\n\nprint(f\"üìÖ First order date: {min_date}\")\nprint(f\"üìÖ Last order date: {max_date}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:51:52.043151Z","iopub.execute_input":"2025-06-12T07:51:52.043464Z","iopub.status.idle":"2025-06-12T07:51:52.156460Z","shell.execute_reply.started":"2025-06-12T07:51:52.043442Z","shell.execute_reply":"2025-06-12T07:51:52.155422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean Product Catalog\nproduct_df.columns = product_df.columns.str.strip()\nproduct_df = product_df.rename(columns={\n    'Model Name - !!!DO NOT CHANGE THE ORDER!!!': 'Model_Name',\n    'SKU': 'SKU',\n    'Typ I': 'Type_1'\n})\nproduct_df['Type_1'] = product_df['Type_1'].astype(str).str.strip()\nproduct_df = product_df[product_df['Type_1'].str.lower() != 'accessories']\nproduct_df['Model_Name'] = product_df['Model_Name'].astype(str).str.strip()\nproduct_df['SKU'] = product_df['SKU'].astype(str).str.strip()\n\n# Clean and Prepare Purchase Data\npurchase_df['created_at'] = pd.to_datetime(purchase_df['created_at'], errors='coerce')\n\n# üéØ Create Fiscal Year column\ndef get_fiscal_year(date):\n    if pd.isnull(date):\n        return None\n    year = date.year\n    if date.month < 4:\n        return f\"{year-1}-{year}\"\n    else:\n        return f\"{year}-{year+1}\"\n\npurchase_df['Fiscal_Year'] = purchase_df['created_at'].apply(get_fiscal_year)\n\n# Keep required columns\ncolumns_to_keep = ['customer_email', 'created_at', 'Fiscal_Year', 'sku_list']\npurchase_df = purchase_df[columns_to_keep]\n\npurchase_df['sku_list'] = purchase_df['sku_list'].astype(str).str.replace(r'\\s+', '', regex=True)\npurchase_df['sku_list'] = purchase_df['sku_list'].str.split(',')\n\ndf_exploded = purchase_df.explode('sku_list')\ndf_exploded = df_exploded[df_exploded['sku_list'].notna() & (df_exploded['sku_list'] != '')]\n\n# Merge with Product Catalog\ndf_exploded['sku_list'] = df_exploded['sku_list'].astype(str)\nmerged_df = pd.merge(df_exploded, product_df, how='inner', left_on='sku_list', right_on='SKU')\nmerged_df = merged_df[merged_df['sku_list'].str.isdigit()]\n\n# Step 1: Count purchases per SKU per Fiscal Year\nsku_counts_yearly = (\n    merged_df\n    .groupby(['Fiscal_Year', 'sku_list'])\n    .size()\n    .reset_index(name='purchase_count')\n)\n\n# Step 2: Top 15 SKUs per Fiscal Year\ntop15_skus_yearly = (\n    sku_counts_yearly\n    .sort_values(['Fiscal_Year', 'purchase_count'], ascending=[True, False])\n    .groupby('Fiscal_Year')\n    .head(15)\n    .reset_index(drop=True)\n)\n\n# Step 3: Repurchase Calculation\nsku_customer_counts = (\n    merged_df\n    .groupby(['Fiscal_Year', 'sku_list', 'customer_email'])\n    .size()\n    .reset_index(name='purchase_count')\n)\n\nsku_customer_counts['repurchased'] = sku_customer_counts['purchase_count'] >= 2\n\n# Filter only SKUs in top 15\ntop_skus_set = set(zip(top15_skus_yearly['Fiscal_Year'], top15_skus_yearly['sku_list']))\nsku_customer_counts = sku_customer_counts[\n    sku_customer_counts.apply(lambda row: (row['Fiscal_Year'], row['sku_list']) in top_skus_set, axis=1)\n]\n\n# Compute repurchase rate\nrepurchase_df = (\n    sku_customer_counts\n    .groupby(['Fiscal_Year', 'sku_list'])\n    .agg(\n        total_customers=('customer_email', 'nunique'),\n        repurchasers=('repurchased', 'sum')\n    )\n    .reset_index()\n)\n\nrepurchase_df['repurchase_rate'] = (\n    (repurchase_df['repurchasers'] / repurchase_df['total_customers']) * 100\n).round(2)\n\n# Step 4: Final Merge for Report\nfinal_df = pd.merge(top15_skus_yearly, repurchase_df, on=['Fiscal_Year', 'sku_list'])\nfinal_df = pd.merge(final_df, product_df, how='left', left_on='sku_list', right_on='SKU')\n\n# Final columns\nfinal_df = final_df[['Fiscal_Year', 'Model_Name', 'SKU', 'Type_1', 'purchase_count', 'total_customers', 'repurchasers', 'repurchase_rate']]\n\n# Save to CSV\nfinal_df.to_csv(\"Top15_SKUs_model_repurchase_by_fiscal_year.csv\", index=False)\nprint(\"‚úÖ File saved as 'Top15_SKUs_model_repurchase_by_fiscal_year.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:46:18.824456Z","iopub.execute_input":"2025-06-12T07:46:18.824790Z","iopub.status.idle":"2025-06-12T07:46:37.093422Z","shell.execute_reply.started":"2025-06-12T07:46:18.824768Z","shell.execute_reply":"2025-06-12T07:46:37.092656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New & Repurchase Customer","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load purchase data\npurchase_data_path = \"/kaggle/input/crm-data-analysis/mehedi-export.csv\"\npurchase_df = pd.read_csv(purchase_data_path, encoding='utf-8', low_memory=False)\n\n# Step 1: Preprocess dates and emails\npurchase_df['created_at'] = pd.to_datetime(purchase_df['created_at'], errors='coerce')\npurchase_df['Year'] = purchase_df['created_at'].dt.year\npurchase_df['customer_email'] = purchase_df['customer_email'].str.strip().str.lower()\n\n# Step 2: Identify New Customers per Year\nnew_customers_by_year = []\nall_previous_customers = set()\n\nfor year in sorted(purchase_df['Year'].dropna().unique()):\n    current_year_df = purchase_df[purchase_df['Year'] == year]\n    current_customers = set(current_year_df['customer_email'].dropna().unique())\n\n    new_customers = current_customers - all_previous_customers\n\n    new_customers_by_year.extend([(year, email) for email in new_customers])\n\n    # Update for next loop\n    all_previous_customers.update(current_customers)\n\nnew_customers_df = pd.DataFrame(new_customers_by_year, columns=['Year', 'customer_email'])\n\nnew_customers_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.024955Z","iopub.status.idle":"2025-06-12T06:30:45.025245Z","shell.execute_reply.started":"2025-06-12T06:30:45.025117Z","shell.execute_reply":"2025-06-12T06:30:45.025131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Identify Repurchasing Customers (multiple orders in same year)\nrepurchase_df = (\n    purchase_df.groupby(['Year', 'customer_email'])\n    .size()\n    .reset_index(name='purchase_count')\n)\n\nrepurchase_df = repurchase_df[repurchase_df['purchase_count'] >= 2]\nrepurchasing_customers_df = repurchase_df[['Year', 'customer_email']]\n\n\nrepurchasing_customers_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.027022Z","iopub.status.idle":"2025-06-12T06:30:45.027370Z","shell.execute_reply.started":"2025-06-12T06:30:45.027236Z","shell.execute_reply":"2025-06-12T06:30:45.027253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Save both lists into a single Excel file (with two sheets)\noutput_filename = \"customer_summary.csv\"\n\n# Saving as Excel for multiple sheets\nwith pd.ExcelWriter(\"customer_summary.xlsx\") as writer:\n    new_customers_df.to_excel(writer, sheet_name='New_Customers', index=False)\n    repurchasing_customers_df.to_excel(writer, sheet_name='Repurchasers', index=False)\n\nprint(\"‚úÖ Saved to 'customer_summary.xlsx' with two sheets: 'New_Customers' and 'Repurchasers'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:30:45.028694Z","iopub.status.idle":"2025-06-12T06:30:45.029006Z","shell.execute_reply.started":"2025-06-12T06:30:45.028874Z","shell.execute_reply":"2025-06-12T06:30:45.028887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}