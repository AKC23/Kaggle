{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12748129,"sourceType":"datasetVersion","datasetId":7312162}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"49a0a54e-570e-4a18-8e4e-2cc418ae9c77","_cell_guid":"dfc68c05-97b9-413f-9703-4953a76852d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:44:54.012920Z","iopub.execute_input":"2025-08-13T23:44:54.013164Z","iopub.status.idle":"2025-08-13T23:44:55.399504Z","shell.execute_reply.started":"2025-08-13T23:44:54.013134Z","shell.execute_reply":"2025-08-13T23:44:55.398468Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/psychai-convo-gen-data/bengali_qa.xlsx\n/kaggle/input/psychai-convo-gen-data/Analyzer Model report and classifications (0-200).xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"3e333a0a-75aa-4844-a594-5a3fce296174","_cell_guid":"050699aa-d82a-45a5-87ba-16f5df4c3b9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:44:55.401332Z","iopub.execute_input":"2025-08-13T23:44:55.401757Z","iopub.status.idle":"2025-08-13T23:44:56.902186Z","shell.execute_reply.started":"2025-08-13T23:44:55.401726Z","shell.execute_reply":"2025-08-13T23:44:56.900934Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# STEP 1: Load your dataset\ndf = pd.read_excel(\"/kaggle/input/psychai-convo-gen-data/Analyzer Model report and classifications.xlsx\")  # Replace with your actual file path\n\ndf.head(3)","metadata":{"_uuid":"9daa4fbd-403b-4006-8d7a-a51fcbafd66e","_cell_guid":"e3b9e980-ef4c-4fd4-9cab-6dcb4e6ac911","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 2: Inspect key columns\nprint(df.columns)","metadata":{"_uuid":"45d367ac-36f2-4431-b2ec-72f2c78902f2","_cell_guid":"4fbda782-b536-43bb-ba94-583c87de75fd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Assume df is loaded with columns:\n# 'Analyzer_Model_Report' (text), 'Classification' (comma-separated labels)\n\n# 1. Clean text\ndef clean_text(text):\n    text = re.sub(r'[\\n\\r\\t]+', ' ', str(text))\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndf['cleaned_text'] = df['Analyzer_Model_Report'].apply(clean_text)\n\n# 2. Prepare multi-label targets\ndf['labels'] = df['Classification'].apply(lambda x: [label.strip() for label in str(x).split(',')])\n\n# 3. MultiLabelBinarizer to encode target labels\nmlb = MultiLabelBinarizer()\nY = mlb.fit_transform(df['labels'])\n\nprint(\"Classes:\", mlb.classes_)\n\n# 4. Split data\nX_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], Y, test_size=0.2, random_state=42)\n\n# 5. Vectorize text with TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000)  # adjust max_features as needed\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# 6. Train One-vs-Rest Logistic Regression classifier\nmodel = OneVsRestClassifier(LogisticRegression(max_iter=1000))\nmodel.fit(X_train_tfidf, y_train)\n\n# 7. Predict & Evaluate\ny_pred = model.predict(X_test_tfidf)\n\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_))","metadata":{"_uuid":"92e75cd9-0f4c-4448-a5d3-3467f65bde12","_cell_guid":"677717c0-ca6f-4d83-ab10-786c36096755","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print formatted labels\nfor idx, label in enumerate(mlb.classes_, 1):  # start index from 1\n    print(f\"{idx}: {label}\")","metadata":{"_uuid":"f8696e3a-aa1f-4290-9276-81031238aa7c","_cell_guid":"94c5c1f1-ba94-4658-81e4-7253833765f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"adc05e9e-2487-4c40-a3ee-99ffa3473744","_cell_guid":"5456453b-e924-4b4c-906d-47bebb90efdc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import HTML\n\n# Get the subset of the 'Analyzer_Model_Report' column\nsubset_df = df['Conversations'][1:3]\n\n# Create HTML with scrollable container\nhtml_content = f\"\"\"\n<div style=\"height: 200px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px;\">\n    {subset_df.to_frame().to_html(index=False, header=True, escape=False)}\n</div>\n\"\"\"\n\n# Display the HTML content\nHTML(html_content)","metadata":{"_uuid":"8144fa45-964c-42d6-8bb0-4e9004e995c4","_cell_guid":"fc940449-a568-4277-83c2-e0f3abf38d1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import HTML\n\n# Get the subset of the 'Analyzer_Model_Report' column\nsubset_df = df['labels'][1:3]\n\n# Create HTML with scrollable container\nhtml_content = f\"\"\"\n<div style=\"height: 200px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px;\">\n    {subset_df.to_frame().to_html(index=False, header=True, escape=False)}\n</div>\n\"\"\"\n\n# Display the HTML content\nHTML(html_content)","metadata":{"_uuid":"b05d1a1f-8ab2-4c64-baf3-43406d15ff02","_cell_guid":"7a416341-ff09-46ab-8f8f-8d116637c759","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"77a64701-9c89-4240-ab90-d931b6012808","_cell_guid":"76756408-7bdb-4639-b73d-001ef91ec9d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"_uuid":"3b3deb5f-5fc7-495d-97b2-ebd8034bf245","_cell_guid":"6f3ebfb9-1a07-45de-a92c-e991eae02c94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport joblib","metadata":{"_uuid":"dadf20f3-6fcc-4052-aea9-0797b722c5aa","_cell_guid":"aac2e71e-adb7-4bfd-a67e-8ad267ccb8a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:45:35.808059Z","iopub.execute_input":"2025-08-13T23:45:35.808495Z","iopub.status.idle":"2025-08-13T23:46:12.490935Z","shell.execute_reply.started":"2025-08-13T23:45:35.808468Z","shell.execute_reply":"2025-08-13T23:46:12.490042Z"}},"outputs":[{"name":"stderr","text":"2025-08-13 23:45:58.489292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755128758.705942      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755128758.775243      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Step 1: Data Preprocessing\n# Load your dataset\ndf = pd.read_excel(\"/kaggle/input/psychai-convo-gen-data/Analyzer Model report and classifications (0-200).xlsx\")  # Replace with your actual file path\n\ndf.head()","metadata":{"_uuid":"082600d5-bf12-4fb6-ab3f-f500ab2424fb","_cell_guid":"4e16edae-b64f-447f-b94a-619cf618e444","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:47:27.085503Z","iopub.execute_input":"2025-08-13T23:47:27.086397Z","iopub.status.idle":"2025-08-13T23:47:27.220007Z","shell.execute_reply.started":"2025-08-13T23:47:27.086362Z","shell.execute_reply":"2025-08-13T23:47:27.219165Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                       Conversations  \\\n0  PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...   \n1  PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...   \n2  PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...   \n3  PsychAI: হাই, আমি PsychAI। আপনি কীভাবে আছেন আজ...   \n4  PsychAI: হ্যালো, আমি PsychAI। আজ আপনি কেমন বোধ...   \n\n                               Analyzer_Model_Report  \\\n0  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n1  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n2  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...   \n3  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n4  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n\n                Classification  \n0  Depression, Anxiety, Stress  \n1              Anxiety, Stress  \n2  Depression, Anxiety, Stress  \n3              Anxiety, Stress  \n4  Depression, Anxiety, Stress  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Conversations</th>\n      <th>Analyzer_Model_Report</th>\n      <th>Classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...</td>\n      <td>১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...</td>\n      <td>Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PsychAI: হাই, আমি PsychAI। আপনি কীভাবে আছেন আজ...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...</td>\n      <td>Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আজ আপনি কেমন বোধ...</td>\n      <td>১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Check the first few rows of the dataset\nprint(\"Data Loaded:\")\nprint(df.head())\n\n# Tokenize the text data (Bengali text)\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Conversations'])\nX = tokenizer.texts_to_sequences(df['Conversations'])\nX = pad_sequences(X, padding='post')  # Padding for sequence uniformity\n\nprint(\"Tokenization and Padding Complete.\")\nprint(f\"Shape of X after padding: {X.shape}\")","metadata":{"_uuid":"b8eb6f36-61ba-4623-b821-bb2c40d84e42","_cell_guid":"61c7201d-2ce4-4b89-86dd-781818f0b907","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:47:34.928878Z","iopub.execute_input":"2025-08-13T23:47:34.929391Z","iopub.status.idle":"2025-08-13T23:47:35.025825Z","shell.execute_reply.started":"2025-08-13T23:47:34.929358Z","shell.execute_reply":"2025-08-13T23:47:35.024647Z"}},"outputs":[{"name":"stdout","text":"Data Loaded:\n                                       Conversations  \\\n0  PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...   \n1  PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...   \n2  PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...   \n3  PsychAI: হাই, আমি PsychAI। আপনি কীভাবে আছেন আজ...   \n4  PsychAI: হ্যালো, আমি PsychAI। আজ আপনি কেমন বোধ...   \n\n                               Analyzer_Model_Report  \\\n0  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n1  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n2  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...   \n3  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n4  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n\n                Classification  \n0  Depression, Anxiety, Stress  \n1              Anxiety, Stress  \n2  Depression, Anxiety, Stress  \n3              Anxiety, Stress  \n4  Depression, Anxiety, Stress  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3327920345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize the text data (Bengali text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Padding for sequence uniformity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    134\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"\"\"DEPRECATED.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"],"ename":"AttributeError","evalue":"'float' object has no attribute 'lower'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Step 2: Word2Vec Embedding\n# Assuming 'tokenized_sentences' is a list of tokenized Bengali sentences (list of list of words)\ntokenized_sentences = [sentence.split() for sentence in df['Conversations']]  # Example tokenization\nmodel_w2v = gensim.models.Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1)\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n\n# Fill the embedding matrix with Word2Vec vectors\nfor word, i in tokenizer.word_index.items():\n    if word in model_w2v.wv:\n        embedding_matrix[i] = model_w2v.wv[word]\n\nprint(\"Word2Vec Embedding Complete.\")\nprint(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")","metadata":{"_uuid":"687a8357-7f4b-49f1-9592-e530da3829b6","_cell_guid":"b5f273f4-eb73-4f8c-8d53-7dabec153455","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-13T23:47:39.519778Z","iopub.execute_input":"2025-08-13T23:47:39.520592Z","iopub.status.idle":"2025-08-13T23:47:39.582143Z","shell.execute_reply.started":"2025-08-13T23:47:39.520544Z","shell.execute_reply":"2025-08-13T23:47:39.580822Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3100940026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 2: Word2Vec Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Assuming 'tokenized_sentences' is a list of tokenized Bengali sentences (list of list of words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Example tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3100940026.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 2: Word2Vec Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Assuming 'tokenized_sentences' is a list of tokenized Bengali sentences (list of list of words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Example tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"],"ename":"AttributeError","evalue":"'float' object has no attribute 'split'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Step 3: Split Data into Train, Validation, and Test\n# Extract labels from the 'Classification' column\n# The 'Classification' column contains comma-separated class labels for each row.\n# We'll split these labels and apply MultiLabelBinarizer for one-hot encoding.\n\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(df['Classification'].str.split(', '))  # Split by comma and space, then one-hot encode\n\n# Check the one-hot encoded labels\nprint(\"One-hot Encoded Labels:\")\nprint(y[:5])  # Print first 5 rows of one-hot encoded labels\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Data Split into Train, Validation, and Test sets.\")\nprint(f\"Shape of X_train: {X_train.shape}, Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}, Shape of y_test: {y_test.shape}\")","metadata":{"_uuid":"720088dd-6d5a-496f-af80-083f0c9d9cba","_cell_guid":"b6ebc14c-8a7d-4b8e-a27f-8f8c6d92a65d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\n# Step 4: Handling Imbalanced Data (Class Weights)\n# Compute class weights for each class (since we are doing multilabel classification)\nclass_weights = {}\nfor i in range(y_train.shape[1]):  # Iterate through each class (column)\n    class_weights[i] = class_weight.compute_class_weight('balanced', \n                                                         classes=np.unique(y_train[:, i]), \n                                                         y=y_train[:, i])\n    class_weights[i] = dict(enumerate(class_weights[i]))\n\nprint(\"Class Weights Computed.\")\nprint(f\"Class Weight Dict for each label: {class_weights}\")","metadata":{"_uuid":"849e75c6-437c-4732-9ae1-dc56bfdb13ab","_cell_guid":"ee7c7272-3c85-43e4-9df5-764c2a6dc2b8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Model Architecture\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n                    output_dim=100, \n                    weights=[embedding_matrix], \n                    input_length=X.shape[1], \n                    trainable=False))  # Embedding layer\nmodel.add(LSTM(128, return_sequences=False))  # LSTM layer\nmodel.add(Dropout(0.3))  # Dropout to prevent overfitting\nmodel.add(Dense(y_train.shape[1], activation='sigmoid'))  # Dense layer with sigmoid for multilabel classification\n\nprint(\"Model Architecture Created.\")\nmodel.summary()","metadata":{"_uuid":"f53873d2-1e15-4f8d-9f4a-830c559e07b6","_cell_guid":"f17a535d-47f9-4f2a-a6a5-888b52d9f17b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Compile Model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(\"Model Compiled.\")","metadata":{"_uuid":"a43b6fa3-fcee-4fa7-9f70-c5d4a2280f35","_cell_guid":"19dc1432-666a-4a3b-a7f1-eb50900e0629","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Train the Model with EarlyStopping\n# We need to modify how we pass class weights to handle multilabel classification correctly.\n\n# For multilabel classification, compute the sample-wise weights for each class\nsample_weights = np.ones_like(y_train)  # Start with all sample weights equal to 1\n\n# Compute class weights for each label (for multilabel classification)\nfor i in range(y_train.shape[1]):  # Iterate through each label (column)\n    class_weights_i = class_weight.compute_class_weight('balanced', \n                                                        classes=np.unique(y_train[:, i]), \n                                                        y=y_train[:, i])\n    class_weights_dict = dict(enumerate(class_weights_i))\n    \n    # Assign the computed class weight for each class\n    sample_weights[:, i] = [class_weights_dict.get(int(label), 1.0) for label in y_train[:, i]]\n\n# Train the model\nmodel.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=64, \n          sample_weight=sample_weights, callbacks=[early_stop])\n\nprint(\"Model Training Complete.\")","metadata":{"_uuid":"23e7fd31-bf05-4fe2-b4b8-32cce3bf94f9","_cell_guid":"7ae29b73-71a6-40a6-b1c0-94baf00c8167","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 8: Model Evaluation\n# Make predictions using the model\ny_pred_prob = model.predict(X_test)\n\n# Convert probabilities to binary predictions (thresholding at 0.5)\ny_pred = (y_pred_prob >= 0.5).astype(int)  # Convert predictions to binary (0 or 1)\n\n# Print classification report (multilabel classification)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=1))\n\n# Plot confusion matrix for each class (multilabel)\n# We'll calculate confusion matrices for each individual class and visualize\nfor i in range(y_train.shape[1]):  # Loop through each class\n    cm = confusion_matrix(y_test[:, i], y_pred[:, i])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n    plt.title(f\"Confusion Matrix for class {mlb.classes_[i]}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\nprint(\"Model Evaluation Complete.\")","metadata":{"_uuid":"86f2de70-11ee-41a8-81b7-56cdcc31d0a7","_cell_guid":"82146b97-560e-4662-a72e-91e63194cfb4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Save the Model\n# Save the model for future use\njoblib.dump(model, 'lstm_model.pkl')\nprint(\"Model Saved.\")","metadata":{"_uuid":"9867d4f7-13e8-47d3-8f14-1d82d019cabe","_cell_guid":"06de02c3-7a82-4896-b654-e60e9fa75a77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"f35a6ca8-dd1e-4c6e-9c48-a57e541a8c4e","_cell_guid":"3cc4962d-0149-4752-bfc1-388f8b173b7a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"516e3689-f395-4ec8-b56e-2e651a5feed5","_cell_guid":"38d2ba9d-ba23-48ff-8a38-e0179984db52","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Automate","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade pip setuptools wheel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T23:50:19.373475Z","iopub.execute_input":"2025-08-13T23:50:19.374653Z","iopub.status.idle":"2025-08-13T23:50:19.379206Z","shell.execute_reply.started":"2025-08-13T23:50:19.374587Z","shell.execute_reply":"2025-08-13T23:50:19.378090Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !pip install gensim==4.0.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T23:50:15.050416Z","iopub.execute_input":"2025-08-13T23:50:15.050760Z","iopub.status.idle":"2025-08-13T23:50:15.054578Z","shell.execute_reply.started":"2025-08-13T23:50:15.050737Z","shell.execute_reply":"2025-08-13T23:50:15.053824Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!pip install autokeras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T23:49:02.887179Z","iopub.execute_input":"2025-08-13T23:49:02.887485Z","iopub.status.idle":"2025-08-13T23:49:08.323789Z","shell.execute_reply.started":"2025-08-13T23:49:02.887467Z","shell.execute_reply":"2025-08-13T23:49:08.322426Z"}},"outputs":[{"name":"stdout","text":"Collecting autokeras\n  Downloading autokeras-2.0.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from autokeras) (25.0)\nRequirement already satisfied: keras-tuner>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from autokeras) (1.4.7)\nRequirement already satisfied: keras-nlp>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from autokeras) (0.18.1)\nRequirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from autokeras) (3.8.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from autokeras) (0.1.9)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (0.1.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (3.14.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (0.16.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->autokeras) (0.4.1)\nRequirement already satisfied: keras-hub==0.18.1 in /usr/local/lib/python3.11/dist-packages (from keras-nlp>=0.8.0->autokeras) (0.18.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (2024.11.6)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.3.12)\nRequirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (2.18.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner>=1.4.0->autokeras) (2.32.4)\nRequirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner>=1.4.0->autokeras) (1.0.5)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->autokeras) (25.3.0)\nRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->autokeras) (1.17.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.0.0->autokeras) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.0.0->autokeras) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner>=1.4.0->autokeras) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner>=1.4.0->autokeras) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner>=1.4.0->autokeras) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner>=1.4.0->autokeras) (2025.6.15)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->autokeras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->autokeras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->autokeras) (0.1.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (4.67.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras>=3.0.0->autokeras) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras>=3.0.0->autokeras) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras>=3.0.0->autokeras) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras>=3.0.0->autokeras) (2024.2.0)\nRequirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (2.18.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras>=3.0.0->autokeras) (2024.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.1.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (1.73.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (2.18.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.45.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp>=0.8.0->autokeras) (3.0.2)\nDownloading autokeras-2.0.0-py3-none-any.whl (122 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.7/122.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: autokeras\nSuccessfully installed autokeras-2.0.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Import necessary libraries for AutoKeras and Hyperparameter Tuning (Optuna)\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport joblib\nimport autokeras as ak","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T23:49:19.789175Z","iopub.execute_input":"2025-08-13T23:49:19.789526Z","iopub.status.idle":"2025-08-13T23:49:21.338205Z","shell.execute_reply.started":"2025-08-13T23:49:19.789496Z","shell.execute_reply":"2025-08-13T23:49:21.337388Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Step 1: Data Preprocessing\n# Load your dataset\ndf = pd.read_excel(\"/kaggle/input/psychai-convo-gen-data/Analyzer Model report and classifications (0-200).xlsx\")  # Replace with your actual file path\n\nprint(df.info())\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T23:53:20.719419Z","iopub.execute_input":"2025-08-13T23:53:20.719772Z","iopub.status.idle":"2025-08-13T23:53:20.854191Z","shell.execute_reply.started":"2025-08-13T23:53:20.719747Z","shell.execute_reply":"2025-08-13T23:53:20.853239Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 3 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   Conversations          195 non-null    object\n 1   Analyzer_Model_Report  195 non-null    object\n 2   Classification         200 non-null    object\ndtypes: object(3)\nmemory usage: 4.8+ KB\nNone\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                       Conversations  \\\n0  PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...   \n1  PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...   \n2  PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...   \n\n                               Analyzer_Model_Report  \\\n0  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n1  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n2  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...   \n\n                Classification  \n0  Depression, Anxiety, Stress  \n1              Anxiety, Stress  \n2  Depression, Anxiety, Stress  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Conversations</th>\n      <th>Analyzer_Model_Report</th>\n      <th>Classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...</td>\n      <td>১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...</td>\n      <td>Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Step 1: Data Preprocessing\n\n# Remove rows where 'Conversations' or 'Analyzer_Model_Report' is empty or NaN\ndf_cleaned = df.dropna(subset=['Conversations', 'Analyzer_Model_Report'])  # Drop rows with NaN in either column\n\n# Remove rows where 'Conversations' or 'Analyzer_Model_Report' is an empty string\ndf_cleaned = df_cleaned[df_cleaned['Conversations'].str.strip() != '']\ndf_cleaned = df_cleaned[df_cleaned['Analyzer_Model_Report'].str.strip() != '']\n\n# Remove rows where 'Classification' contains the error message 'Error: can only concatenate str (not \"float\") to str'\ndf_cleaned = df_cleaned[df_cleaned['Classification'] != 'Error: can only concatenate str (not \"float\") to str']\n\n# Display the cleaned DataFrame info\nprint(\"Cleaned DataFrame:\")\nprint(df_cleaned.info())\n\n# Show the first few rows of the cleaned DataFrame\ndf_cleaned.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:11:05.643519Z","iopub.execute_input":"2025-08-14T00:11:05.643884Z","iopub.status.idle":"2025-08-14T00:11:05.663479Z","shell.execute_reply.started":"2025-08-14T00:11:05.643860Z","shell.execute_reply":"2025-08-14T00:11:05.662697Z"}},"outputs":[{"name":"stdout","text":"Cleaned DataFrame:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 195 entries, 0 to 199\nData columns (total 3 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   Conversations          195 non-null    object\n 1   Analyzer_Model_Report  195 non-null    object\n 2   Classification         195 non-null    object\ndtypes: object(3)\nmemory usage: 6.1+ KB\nNone\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                                       Conversations  \\\n0  PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...   \n1  PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...   \n2  PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...   \n3  PsychAI: হাই, আমি PsychAI। আপনি কীভাবে আছেন আজ...   \n4  PsychAI: হ্যালো, আমি PsychAI। আজ আপনি কেমন বোধ...   \n\n                               Analyzer_Model_Report  \\\n0  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n1  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n2  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...   \n3  ১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...   \n4  ১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...   \n\n                Classification  \n0  Depression, Anxiety, Stress  \n1              Anxiety, Stress  \n2  Depression, Anxiety, Stress  \n3              Anxiety, Stress  \n4  Depression, Anxiety, Stress  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Conversations</th>\n      <th>Analyzer_Model_Report</th>\n      <th>Classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি যা অভিজ্ঞতা...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আপনি অনেক জটিল স...</td>\n      <td>১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...</td>\n      <td>Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PsychAI: হেলো, আমি PsychAI বলছি। আমি কীভাবে আপ...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস:\\r\\nক্লায়...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PsychAI: হাই, আমি PsychAI। আপনি কীভাবে আছেন আজ...</td>\n      <td>১. CBT Report:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন্...</td>\n      <td>Anxiety, Stress</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PsychAI: হ্যালো, আমি PsychAI। আজ আপনি কেমন বোধ...</td>\n      <td>১. CBT রিপোর্ট:\\r\\nপ্রাসঙ্গিক ইতিহাস: ক্লায়েন...</td>\n      <td>Depression, Anxiety, Stress</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# List of columns to examine\ncolumns_to_check = ['Classification']\n\n# Loop through the columns and get unique values and their counts\nfor col in columns_to_check:\n    if col in df_cleaned.columns:  # Ensure the column exists in the DataFrame\n        print(f\"Unique values in '{col}' column:\")\n        print(df_cleaned[col].value_counts(dropna=False))  # dropna=False to include NaN counts\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:11:09.157073Z","iopub.execute_input":"2025-08-14T00:11:09.157359Z","iopub.status.idle":"2025-08-14T00:11:09.164890Z","shell.execute_reply.started":"2025-08-14T00:11:09.157339Z","shell.execute_reply":"2025-08-14T00:11:09.163883Z"}},"outputs":[{"name":"stdout","text":"Unique values in 'Classification' column:\nClassification\nAnxiety, Stress                                                                                            57\nDepression, Anxiety, Stress                                                                                47\nAnxiety, Depression, Stress                                                                                12\nAnxiety, Stress, Depression                                                                                10\nAnxiety                                                                                                     5\nDepression, Stress, Anxiety                                                                                 5\nStress, Anxiety                                                                                             4\nDepression, Stress                                                                                          4\nDepression, Anxiety, Suicidal Tendency                                                                      4\nPost-Traumatic Stress Disorder (PTSD), Anxiety, Stress                                                      3\nDepression, Anxiety, Post-Traumatic Stress Disorder (PTSD)                                                  3\nAnxiety, Depression, Suicidal Tendency                                                                      3\nAnxiety, Dissociative Disorders, Stress                                                                     2\nPost-Traumatic Stress Disorder (PTSD), Anxiety, Depression                                                  2\nAnxiety, Post-Traumatic Stress Disorder (PTSD)                                                              2\nAnxiety, Depression                                                                                         2\nDepression, Suicidal Tendency, Stress                                                                       2\nAnxiety, Stress, Post-Traumatic Stress Disorder (PTSD)                                                      1\nDepression, Stress, Post-Traumatic Stress Disorder (PTSD)                                                   1\nDepression, Stress, Suicidal Tendency                                                                       1\nBinge Eating Disorder, Stress, Anxiety                                                                      1\nAnxiety, Stress, Suicidal Tendency                                                                          1\nPost-Traumatic Stress Disorder (PTSD), Depression, Paranoid Personality Disorder (PPD)                      1\nDepression, Post-Traumatic Stress Disorder (PTSD), Stress                                                   1\nBinge Eating Disorder, Depression, Anxiety                                                                  1\nPost-Traumatic Stress Disorder (PTSD), Depression, Anxiety, Paranoid Personality Disorder (PPD), Stress     1\nPost-Traumatic Stress Disorder (PTSD), Depression, Anxiety, Paranoid Personality Disorder (PPD)             1\nBinge Eating Disorder, Depression, Anxiety, Stress                                                          1\nSleep Awake Disorder, Stress, Anxiety, Depression                                                           1\nDepression, Suicidal Tendency, Stress, Anxiety                                                              1\nSchizophrenia Spectrum Disorders, Depression, Anxiety, Stress, Sleep Awake Disorder                         1\nBipolar Disorder, Depression, Anxiety, Stress                                                               1\nDepression, Anxiety, Stress, Post-Traumatic Stress Disorder (PTSD)                                          1\nDepression, Post-Traumatic Stress Disorder (PTSD), Anxiety, Paranoid Personality Disorder (PPD)             1\nDepression, Anxiety, Suicidal Tendency, Stress                                                              1\nDepression, Suicidal Tendency, Anxiety, Stress, Sleep Awake Disorder                                        1\nBipolar Disorder, Stress, Anxiety                                                                           1\nAnxiety, Sleep Awake Disorder, Stress                                                                       1\nDepression, Anxiety, Stress, Sleep Awake Disorder                                                           1\nAnxiety, Binge Eating Disorder, Suicidal Tendency                                                           1\nBipolar Disorder, Depression, Stress, Anxiety                                                               1\nAnxiety, Paranoid Personality Disorder (PPD)                                                                1\nDepression, Anxiety, Binge Eating Disorder                                                                  1\nAnxiety, Suicidal Tendency                                                                                  1\nDepression, Anxiety                                                                                         1\nName: count, dtype: int64\n\n--------------------------------------------------\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"df = df_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:12:05.472636Z","iopub.execute_input":"2025-08-14T00:12:05.473323Z","iopub.status.idle":"2025-08-14T00:12:05.477633Z","shell.execute_reply.started":"2025-08-14T00:12:05.473296Z","shell.execute_reply":"2025-08-14T00:12:05.476663Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Tokenize the text data (Bengali text)\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Conversations'])\n\nprint(tokenizer)\n\n# Convert the text data into sequences\nX = tokenizer.texts_to_sequences(df['Conversations'])\nX = pad_sequences(X, padding='post')  # Padding for sequence uniformity\n\n# Check the shape after padding\nprint(f\"Shape of X after padding: {X.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:12:05.687299Z","iopub.execute_input":"2025-08-14T00:12:05.688095Z","iopub.status.idle":"2025-08-14T00:12:05.858552Z","shell.execute_reply.started":"2025-08-14T00:12:05.688068Z","shell.execute_reply":"2025-08-14T00:12:05.857566Z"}},"outputs":[{"name":"stdout","text":"<keras.src.legacy.preprocessing.text.Tokenizer object at 0x7e878ed5f190>\nShape of X after padding: (195, 616)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Apply MultiLabelBinarizer for multilabel classification\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(df['Classification'].str.split(', '))  # One-hot encode the labels\n\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:12:10.537178Z","iopub.execute_input":"2025-08-14T00:12:10.537512Z","iopub.status.idle":"2025-08-14T00:12:10.545885Z","shell.execute_reply.started":"2025-08-14T00:12:10.537486Z","shell.execute_reply":"2025-08-14T00:12:10.545050Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, ..., 0, 1, 0],\n       [1, 0, 0, ..., 0, 1, 0],\n       [1, 0, 0, ..., 0, 1, 0],\n       ...,\n       [1, 0, 0, ..., 0, 1, 0],\n       [1, 0, 0, ..., 0, 1, 0],\n       [1, 0, 0, ..., 0, 1, 0]])"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# List of symptoms you want to check\ntarget_symptoms = [\n    \"Depression\", \"Suicidal Tendency\", \"Bipolar Disorder\", \"Stress\", \"Anxiety\", \n    \"Post-Traumatic Stress Disorder (PTSD)\", \"Sleep Awake Disorder\", \"Binge Eating Disorder\", \n    \"Dissociative Disorders\", \"Schizophrenia Spectrum Disorders\", \"Paranoid Personality Disorder (PPD)\"\n]\n\n# Get the class labels from the MultiLabelBinarizer\nlabels = mlb.classes_\n\n# Count the occurrences of each label across all rows\nlabel_counts = np.sum(y, axis=0)\n\n# Create a dictionary of label occurrences\nlabel_occurrences = {labels[i]: label_counts[i] for i in range(len(labels))}\n\n# Check if all target symptoms are present in the labels\nmissing_symptoms = [symptom for symptom in target_symptoms if symptom not in labels]\nif missing_symptoms:\n    print(\"These symptoms are missing in the labels:\")\n    print(missing_symptoms)\nelse:\n    print(\"All target symptoms are present in the labels.\")\n\n# Print the occurrences for each of the target symptoms\nprint(\"\\nTarget Symptoms Occurrences:\")\nfor symptom in target_symptoms:\n    if symptom in label_occurrences:\n        print(f\"{symptom}: {label_occurrences[symptom]}\")\n    else:\n        print(f\"{symptom}: Not present in labels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:14:12.475360Z","iopub.execute_input":"2025-08-14T00:14:12.476339Z","iopub.status.idle":"2025-08-14T00:14:12.484084Z","shell.execute_reply.started":"2025-08-14T00:14:12.476308Z","shell.execute_reply":"2025-08-14T00:14:12.483059Z"}},"outputs":[{"name":"stdout","text":"All target symptoms are present in the labels.\n\nTarget Symptoms Occurrences:\nDepression: 114\nSuicidal Tendency: 16\nBipolar Disorder: 3\nStress: 165\nAnxiety: 185\nPost-Traumatic Stress Disorder (PTSD): 18\nSleep Awake Disorder: 5\nBinge Eating Disorder: 5\nDissociative Disorders: 2\nSchizophrenia Spectrum Disorders: 1\nParanoid Personality Disorder (PPD): 5\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data into train, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Compute class weights for multilabel classification\nclass_weights = {}\nfor i in range(y_train.shape[1]):\n    class_weights[i] = class_weight.compute_class_weight('balanced', \n                                                         classes=np.unique(y_train[:, i]), \n                                                         y=y_train[:, i])\n    class_weights[i] = dict(enumerate(class_weights[i]))\n\n# Step 2: AutoKeras Model Selection (AutoML Integration)\n# AutoKeras will automatically search for the best architecture\n\n# Set up AutoKeras Text Model\ntext_classifier = ak.TextClassifier(max_trials=5,  # Number of models to try\n                                   overwrite=True,  # Overwrite existing models\n                                   loss='binary_crossentropy',  # Appropriate for multilabel classification\n                                   metrics=['accuracy'])\n\n# Train AutoKeras model\ntext_classifier.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n\n# Step 3: Evaluate AutoKeras Model\n# Get the best model and evaluate it\nbest_model = text_classifier.export_model()\n\n# Evaluate the model\ny_pred_prob = best_model.predict(X_test)\n\n# Convert probabilities to binary predictions (thresholding at 0.5)\ny_pred = (y_pred_prob >= 0.5).astype(int)\n\n# Classification report and confusion matrix\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=1))\n\n# Plot confusion matrix for each class (multilabel)\nfor i in range(y_train.shape[1]):\n    cm = confusion_matrix(y_test[:, i], y_pred[:, i])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n    plt.title(f\"Confusion Matrix for class {mlb.classes_[i]}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\n# Step 4: Save the Best Model\njoblib.dump(best_model, 'best_lstm_model.pkl')\nprint(\"Best Model Saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:14:23.658605Z","iopub.execute_input":"2025-08-14T00:14:23.658961Z","iopub.status.idle":"2025-08-14T00:14:25.113946Z","shell.execute_reply.started":"2025-08-14T00:14:23.658936Z","shell.execute_reply":"2025-08-14T00:14:25.112694Z"}},"outputs":[{"name":"stderr","text":"2025-08-14 00:14:24.952425: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1480847136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train AutoKeras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Step 3: Evaluate AutoKeras Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autokeras/tasks/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 applicable).\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         history = super().fit(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         )\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hyper_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36m_analyze_data\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0manalyser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalysers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0manalyser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autokeras/analysers/input_analysers.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;34m\"Expect the data to TextInput to have shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"(batch_size, 1), but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expect the data to TextInput to have shape (batch_size, 1), but got input shape [64, 616]."],"ename":"ValueError","evalue":"Expect the data to TextInput to have shape (batch_size, 1), but got input shape [64, 616].","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"_uuid":"3d07fd20-70d7-4b9f-985c-1c528b870e56","_cell_guid":"2096555b-f055-4e5c-8a71-8d81d54c177d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Data Preprocessing\n# Load your dataset\ndf = pd.read_excel(\"/kaggle/input/psychai-convo-gen-data/Analyzer Model report and classifications (0-200).xlsx\")  # Replace with your actual file path\n\n# Fill missing values if necessary (you can remove this step if you already cleaned it)\ndf['Conversations'] = df['Conversations'].fillna('')\n\n# Prepare text data (use the raw text for AutoKeras, not tokenized/padded data)\nX = df['Conversations'].values  # Use raw text data\n\n# Apply MultiLabelBinarizer for multilabel classification (same as before)\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(df['Classification'].str.split(', '))  # One-hot encode the labels\n\n# Split data into train, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Compute class weights for multilabel classification\nclass_weights = {}\nfor i in range(y_train.shape[1]):\n    class_weights[i] = class_weight.compute_class_weight('balanced', \n                                                         classes=np.unique(y_train[:, i]), \n                                                         y=y_train[:, i])\n    class_weights[i] = dict(enumerate(class_weights[i]))\n\n# Step 2: AutoKeras Model Selection (AutoML Integration)\n# AutoKeras will automatically search for the best architecture\n\n# Set up AutoKeras Text Model\ntext_classifier = ak.TextClassifier(max_trials=5,  # Number of models to try\n                                   overwrite=True,  # Overwrite existing models\n                                   loss='binary_crossentropy',  # Appropriate for multilabel classification\n                                   metrics=['accuracy'])\n\n# Train AutoKeras model (fit directly on raw text data)\ntext_classifier.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n\n# Step 3: Evaluate AutoKeras Model\n# Get the best model and evaluate it\nbest_model = text_classifier.export_model()\n\n# Evaluate the model\ny_pred_prob = best_model.predict(X_test)\n\n# Convert probabilities to binary predictions (thresholding at 0.5)\ny_pred = (y_pred_prob >= 0.5).astype(int)\n\n# Classification report and confusion matrix\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=1))\n\n# Plot confusion matrix for each class (multilabel)\nfor i in range(y_train.shape[1]):\n    cm = confusion_matrix(y_test[:, i], y_pred[:, i])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n    plt.title(f\"Confusion Matrix for class {mlb.classes_[i]}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\n# Step 4: Save the Best Model\njoblib.dump(best_model, 'best_lstm_model.pkl')\nprint(\"Best Model Saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T00:16:54.298749Z","iopub.execute_input":"2025-08-14T00:16:54.299111Z","execution_failed":"2025-08-14T00:20:01.711Z"}},"outputs":[{"name":"stdout","text":"\nSearch: Running Trial #1\n\nValue             |Best Value So Far |Hyperparameter\n0                 |0                 |classification_head_1/dropout\nadam_weight_decay |adam_weight_decay |optimizer\n2e-05             |2e-05             |learning_rate\n512               |512               |text_block_1/bert_block_1/max_sequence_length\n\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: ['keras_tensor']\nReceived: inputs=Tensor(shape=(None,))\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"https://chatgpt.com/share/689d0c53-e9fc-8007-a2e3-8bb9df3fa3e1 ","metadata":{}},{"cell_type":"code","source":"","metadata":{"_uuid":"f2815645-fe13-45e5-b02a-fa1c922bfee5","_cell_guid":"4145f982-3374-4f62-801c-913b881336f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}